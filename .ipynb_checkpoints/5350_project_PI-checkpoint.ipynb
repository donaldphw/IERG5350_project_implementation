{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "#     \"4x4\": [\n",
    "#         \"SFFF\",\n",
    "#         \"FHFH\",\n",
    "#         \"FFFH\",\n",
    "#         \"HFFG\"\n",
    "#     ],\n",
    "    \"8x8\": [\n",
    "        \"SWWOOWWD\",\n",
    "        \"WWWOOWWW\",\n",
    "        \"WWWWWWWW\",\n",
    "        \"WWWWWWWW\",\n",
    "        \"OOWWWWWW\",\n",
    "        \"WWWWWWWW\",\n",
    "        \"WWWWWWOO\",\n",
    "        \"DWWWWWWD\"\n",
    "    ],\n",
    "    \"16x16\": [\n",
    "        \"SWWWWWWWWWWOOWWD\",\n",
    "        \"WWWWWWWWWWWOOWWW\",\n",
    "        \"WWWWWWWWWWWWWWWW\",\n",
    "        \"WWWWOOOWWWWWWWWW\",\n",
    "        \"WWWWOOOWWWWWWWWW\",\n",
    "        \"WWWWWWWWWWWWWWWW\",\n",
    "        \"OOOWWWWWWWWWWOOO\",\n",
    "        \"OOOWWWWWWWWWWOOO\",\n",
    "        \"WWWWWWWWWWWWWWWW\",\n",
    "        \"WWWWWWOOOWWWWWWW\",\n",
    "        \"WWWWWWOOOWWWWWWW\",\n",
    "        \"WWWWWWWWWWWWWWWW\",\n",
    "        \"OOOWWWWWWWWWWWWW\",\n",
    "        \"OOOWWWWWWWWWWWWW\",\n",
    "        \"WWWWWWWWWWOOOOOO\",\n",
    "        \"DWWWWWWWWWWWWWWD\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "rewards_dict = {\n",
    "    \"8x8\":\n",
    "    {\n",
    "        7 : 2.0,\n",
    "        56 : 4.0,\n",
    "        63 : 10.0\n",
    "    },\n",
    "    \"16x16\":\n",
    "    {\n",
    "        15: 4.0,\n",
    "        240: 8.0,\n",
    "        255 : 20.0\n",
    "    }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "def get_destination(MAP):\n",
    "            destination = []\n",
    "            row = len(MAP)\n",
    "            col = len(MAP[row-1])\n",
    "\n",
    "            for i in range(row):\n",
    "                for j in range(col):\n",
    "\n",
    "                    newletter = MAP[i][j]\n",
    "                    if newletter == \"D\":\n",
    "\n",
    "                        destination.append(i*col + j)\n",
    "            return destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SailingEnv():\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "#     def __init__(self, desc=None, map_name=\"8x8\", is_slippery=True):\n",
    "    def __init__(self, config):\n",
    "#         if desc is None and map_name is None:\n",
    "#             desc = generate_random_map()\n",
    "#         elif desc is None:\n",
    "#             desc = MAPS[map_name]\n",
    "        \n",
    "        self.map_name = config[\"map_name\"]\n",
    "        desc = MAPS[self.map_name]\n",
    "        is_slippery=config[\"is_slippery\"]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.total_steps = config[\"total_steps\"] \n",
    "        self.destinations = get_destination(desc)\n",
    "        self.total_destinations = len(self.destinations)\n",
    "        self.destinations_dict = {D: False for D in self.destinations}\n",
    "        self.num_reached_destinations = 0\n",
    "        \n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "            \n",
    "        self.desc = desc = np.asarray(desc, dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, self.total_destinations)\n",
    "        \n",
    "        self.nA = 4\n",
    "        self.nS = nrow * ncol\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.seed()\n",
    "        self.isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        self.isd /= self.isd.sum()\n",
    "\n",
    "        self.P = {s: {a: [] for a in range(self.nA)} for s in range(self.nS)}\n",
    "        \n",
    "        \n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "        \n",
    "        \n",
    "        def seed(self, seed=None):\n",
    "            self.np_random, seed = seeding.np_random(seed)\n",
    "            return [seed]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        def update_reached_destinations(newstate):\n",
    "            if newstate in self.destinations_dict:\n",
    "                if self.destinations_dict[newstate] == False:\n",
    "                    self.destinations_dict[newstate] = True\n",
    "                    self.num_reached_destinations +=1\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            \n",
    "        def get_reward():\n",
    "            for key, value in self.destinations_dict.items():\n",
    "                if value == False:\n",
    "                    return float(0.0)\n",
    "            return float(1.0)\n",
    "            \n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            \n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            is_updated_destinations = update_reached_destinations(newstate)\n",
    "            \n",
    "                \n",
    "            done = bytes(newletter) in b'OD'\n",
    "#             done = self.current_step == self.total_steps\n",
    "\n",
    "        \n",
    "\n",
    "            if is_updated_destinations == True:\n",
    "                done =  self.num_reached_destinations == self.total_destinations\n",
    "            \n",
    "            reward = 0.0\n",
    "            is_get_reward = float(newletter == b'D')\n",
    "            if is_get_reward == True and newstate in rewards_dict[self.map_name]:\n",
    "#                 if is_updated_destinations == True:\n",
    "                reward = rewards_dict[self.map_name][newstate]\n",
    "#             reward = float(newletter == b'D')\n",
    "#             reward = get_reward()\n",
    "#             reward = float(self.num_reached_destinations == self.total_destinations)\n",
    "            return newstate, reward, done\n",
    "        \n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = self.P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'OD':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                                li.append((\n",
    "                                    1. / 3.,\n",
    "                                    *update_probability_matrix(row, col, b)\n",
    "                                ))\n",
    "                        else:\n",
    "                            li.append((\n",
    "                                1., *update_probability_matrix(row, col, a)\n",
    "                            ))\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    def update_reached_destination(self, newstate):\n",
    "        if newstate in self.destinations_dict:\n",
    "            if self.destinations_dict[newstate] == False:\n",
    "                self.destinations_dict[newstate] = True\n",
    "                self.num_reached_destinations +=1\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        \n",
    "#         is_updated_destinations = self.update_reached_destination(s)\n",
    "#         r = float(self.num_reached_destinations == self.total_destinations)\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        \n",
    "        \n",
    "#         if is_updated_destinations == True:\n",
    "#             d =  self.num_reached_destinations == self.total_destinations\n",
    "\n",
    "        if self.current_step == self.total_steps:\n",
    "            d =  True\n",
    "        if d != True:\n",
    "            self.current_step = self.current_step + 1\n",
    "\n",
    "            \n",
    "        return (int(s), r, d, {\"prob\": p})\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        self.num_reached_destinations = 0\n",
    "\n",
    "        self.destinations_dict = {D: False for D in self.destinations}\n",
    "        return int(self.s)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format(\n",
    "                [\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 1000,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map_name = \"16x16\",  \n",
    "    is_slippery = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingEnv(environment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation space: Discrete(256)\n",
      "Current action space: Discrete(4)\n",
      "0 in action space? True\n",
      "5 in action space? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation space: {}\".format(env.observation_space))\n",
    "print(\"Current action space: {}\".format(env.action_space))\n",
    "print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n",
    "print(\"5 in action space? {}\".format(env.action_space.contains(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SWWWWWWWWWWOOWWD\n",
      "WWWWWWWWWWWOOWWW\n",
      "WWWWWWWWWWWWWWWW\n",
      "WWWW\u001b[41mO\u001b[0mOOWWWWWWWWW\n",
      "WWWWOOOWWWWWWWWW\n",
      "WWWWWWWWWWWWWWWW\n",
      "OOOWWWWWWWWWWOOO\n",
      "OOOWWWWWWWWWWOOO\n",
      "WWWWWWWWWWWWWWWW\n",
      "WWWWWWOOOWWWWWWW\n",
      "WWWWWWOOOWWWWWWW\n",
      "WWWWWWWWWWWWWWWW\n",
      "OOOWWWWWWWWWWWWW\n",
      "OOOWWWWWWWWWWWWW\n",
      "WWWWWWWWWWOOOOOO\n",
      "DWWWWWWWWWWWWWWD\n",
      "Current step: 22\n",
      "Current observation: 52\n",
      "Current reward: 0.0\n",
      "Whether we are done: True\n",
      "info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # take random action\n",
    "    # [TODO] Uncomment next line\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "    # render the environment\n",
    "    env.render()  # [TODO] Uncomment this line\n",
    "\n",
    "    print(\"Current step: {}\\nCurrent observation: {}\\nCurrent reward: {}\\n\"\n",
    "          \"Whether we are done: {}\\ninfo: {}\".format(\n",
    "        env.current_step, obs, reward, done, info\n",
    "    ))\n",
    "    wait(sleep=0.4)\n",
    "    # [TODO] terminate the loop if done\n",
    "    if done:\n",
    "        break\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='SailingEnv', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = SailingEnv(environment_config)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name=\"SailingEnv\", model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = SailingEnv(environment_config)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.P[state][act]\n",
    "#         print(transitions)\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('SailingEnv')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PolicyItertaionTrainer(TabularRLTrainerAbstract):\n",
    "    def random_policy(ops):\n",
    "            return np.random.choice(self.action_dim, size=(self.env.observation_space.n))\n",
    "    def __init__(self, gamma=1.0, eps=1e-10, env_name='SailingEnv'):\n",
    "        super(PolicyItertaionTrainer, self).__init__(env_name)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # value function convergence criterion\n",
    "        self.eps = eps\n",
    "\n",
    "        # build the value table for each possible observation\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # [TODO] you need to implement a random policy at the beginning.\n",
    "        # It is a function that take an integer (state or say observation)\n",
    "        # as input and return an interger (action).\n",
    "        # remember, you can use self.action_dim to get the dimension (range)\n",
    "        # of the action, which is an integer in range\n",
    "        # [0, ..., self.action_dim - 1]\n",
    "        # hint: generating random action at each call of policy may lead to\n",
    "        #  failure of convergence, try generate random actions at initializtion\n",
    "        #  and fix it during the training.\n",
    "        policy_array = np.random.randint(self.action_dim, size = (self.obs_dim))\n",
    "#         def random_policy(state):\n",
    "#             return random_policy_array[state]\n",
    "        \n",
    "#         self.policy = random_policy\n",
    "        self.policy = lambda obs: policy_array[obs]\n",
    "        # test your random policy\n",
    "        test_random_policy(self.policy, self.env)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        # hint: the value function is equivalent to self.table,\n",
    "        #  a numpy array with length 64.\n",
    "\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "        self.update_value_function()\n",
    "        self.update_policy()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        count = 0  # count the steps of value updates\n",
    "        while True:\n",
    "            old_table = self.table.copy()\n",
    "\n",
    "            for state in range(self.obs_dim):\n",
    "                \n",
    "                act = self.policy(state)\n",
    "                transition_list = self._get_transitions(state, act)\n",
    "                state_value = 0\n",
    "                for transition in transition_list:\n",
    "                    \n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    \n",
    "                    # [TODO] what is the right state value?\n",
    "                    # hint: you should use reward, self.gamma, old_table, prob,\n",
    "                    # and next_state to compute the state value\n",
    "#                     pass\n",
    "                    state_value += prob * (reward + self.gamma * old_table[next_state])\n",
    "                # update the state value\n",
    "                    \n",
    "                self.table[state] = state_value\n",
    "            \n",
    "\n",
    "            # [TODO] Compare the old_table and current table to\n",
    "            #  decide whether to break the value update process.\n",
    "            # hint: you should use self.eps, old_table and self.table\n",
    "            should_break = True if np.sum(np.abs(old_table - self.table)) < self.eps else False\n",
    "\n",
    "            if should_break:\n",
    "                break\n",
    "            count += 1\n",
    "            if count % 20000 == 0:\n",
    "                # disable this part if you think debug message annoying.\n",
    "                \n",
    "                print(\"[DEBUG]\\tUpdated values for {} steps. \"\n",
    "                      \"Difference between new and old table is: {}\".format(\n",
    "                    count, np.sum(np.abs(old_table - self.table))\n",
    "                ))\n",
    "#             if count > 4000:\n",
    "#                 print(\"[HINT] Are you sure your codes is OK? It shouldn't be \"\n",
    "#                       \"so hard to update the value function. You already \"\n",
    "#                       \"use {} steps to update value function within \"\n",
    "#                       \"single iteration.\".format(count))\n",
    "#             if count > 6000:\n",
    "#                 raise ValueError(\"Clearly your code has problem. Check it!\")\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"You need to define a new policy function, given current\n",
    "        value function. The best action for a given state is the one that\n",
    "        has greatest expected return.\n",
    "\n",
    "        To optimize computing efficiency, we introduce a policy table,\n",
    "        which take state as index and return the action given a state.\n",
    "        \"\"\"\n",
    "        policy_table = np.zeros([self.obs_dim, ], dtype=np.int)\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            \n",
    "            # [TODO] assign the action with greatest \"value\"\n",
    "            # to policy_table[state]\n",
    "            # hint: what is the proper \"value\" here?\n",
    "            #  you should use table, gamma, reward, prob,\n",
    "            #  next_state and self._get_transitions() function\n",
    "            #  as what we done at self.update_value_function()\n",
    "            #  Bellman equation may help.\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            best_action = np.argmax(state_action_values)\n",
    "            \n",
    "            \n",
    "            policy_table[state] = best_action\n",
    "        self.policy = lambda obs: policy_table[obs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "\n",
    "class ValueIterationTrainer(PolicyItertaionTrainer):\n",
    "    \"\"\"Note that we inherate Policy Iteration Trainer, to resue the\n",
    "    code of update_policy(). It's same since it get optimal policy from\n",
    "    current state-value table (self.table).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, env_name='SailingEnv'):\n",
    "        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "#         self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # In value iteration, we do not explicit require a\n",
    "        # policy instance to run. We update value function\n",
    "        # directly based on the transitions. Therefore, we\n",
    "        # don't need to run self.update_policy() in each step.\n",
    "        self.update_value_function()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.table.copy()\n",
    "        for state in range(self.obs_dim):\n",
    "            state_value = 0\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            # [TODO] what should be de right state value?\n",
    "            # hint: try to compute the state_action_values first\n",
    "                state_value = np.max(state_action_values)\n",
    "#                 print(state_value)\n",
    "                self.table[state] = state_value\n",
    "\n",
    "\n",
    "        # Till now the one step value update is finished.\n",
    "        # You can see that we do not use a inner loop to update\n",
    "        # the value function like what we did in policy iteration.\n",
    "        # This is because to compute the state value, which is\n",
    "        # a expectation among all possible action given by a\n",
    "        # specified policy, we **pretend** already own the optimal\n",
    "        # policy (the max operation).\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().evaluate()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_pi_config = dict(\n",
    "    max_iteration=500,\n",
    "    evaluate_interval=1,\n",
    "    gamma=0.99,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def policy_iteration(train_config=None):\n",
    "    config = default_pi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "        \n",
    "    trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "    old_policy_result = {\n",
    "        obs: -1 for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # [TODO] compare the new policy with old policy to check whether\n",
    "        #  should we stop. If new and old policy have same output given any\n",
    "        #  observation, them we consider the algorithm is converged and\n",
    "        #  should be stopped.\n",
    "        new_policy_result = {\n",
    "             obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "        }\n",
    "        \n",
    "        should_stop = True if new_policy_result == old_policy_result else False\n",
    "        if should_stop:\n",
    "            print(\"We found policy is not changed anymore at \"\n",
    "                  \"itertaion {}. Current mean episode reward \"\n",
    "                  \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "            break\n",
    "        old_policy_result = new_policy_result\n",
    "#         print(old_policy_result)\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "#             if i > 20:\n",
    "#                 print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "#                       \"({}) iterations to train a policy iteration \"\n",
    "#                       \"agent.\".format(i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 7.412.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 7.448.\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 7.88.\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 8.164.\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 9.86.\n",
      "[INFO]\tIn 5 iteration, current mean episode reward is 15.28.\n",
      "[INFO]\tIn 6 iteration, current mean episode reward is 17.16.\n",
      "[INFO]\tIn 7 iteration, current mean episode reward is 18.308.\n",
      "[INFO]\tIn 8 iteration, current mean episode reward is 17.452.\n",
      "[INFO]\tIn 9 iteration, current mean episode reward is 17.748.\n",
      "We found policy is not changed anymore at itertaion 10. Current mean episode reward is 17.748. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# It may be confusing to call a trainer agent. But that's what we normally do.\n",
    "pi_agent = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.88677799,  3.99705023,  4.1181418 ,  4.25029135,  4.39809972,\n",
       "        4.55806158,  4.71111685,  4.83113491,  4.88716842,  4.9109776 ,\n",
       "        4.9053692 ,  0.        ,  0.        ,  4.65316965,  4.65173601,\n",
       "        0.        ,  3.89428691,  3.99708138,  4.11078443,  4.23127969,\n",
       "        4.37141362,  4.54312938,  4.73385991,  4.92149943,  5.01145526,\n",
       "        5.0654035 ,  5.04840835,  0.        ,  0.        ,  4.79560844,\n",
       "        4.79126407,  4.76348267,  3.90950114,  4.00453318,  4.10750102,\n",
       "        4.20035469,  4.30547883,  4.47513724,  4.71241374,  5.07104447,\n",
       "        5.19932512,  5.28984397,  5.32743443,  5.30218807,  5.21571342,\n",
       "        5.08771306,  4.95989091,  4.88004924,  3.93293883,  4.03036661,\n",
       "        4.13583367,  4.19158904,  0.        ,  0.        ,  0.        ,\n",
       "        5.24597688,  5.39464223,  5.50307066,  5.55170866,  5.52408872,\n",
       "        5.41529104,  5.24170799,  5.06221013,  4.94808786,  3.95469101,\n",
       "        4.07286531,  4.23373924,  4.36559659,  0.        ,  0.        ,\n",
       "        0.        ,  5.43121294,  5.59835315,  5.72962081,  5.79620019,\n",
       "        5.77266308,  5.64417614,  5.3809595 ,  5.13843181,  4.98390765,\n",
       "        3.95635583,  4.07791023,  4.32808259,  4.80375231,  5.07403399,\n",
       "        5.26178299,  5.42906419,  5.613891  ,  5.80387276,  5.96793396,\n",
       "        6.06195911,  6.0525421 ,  5.91560995,  5.42005375,  5.12783586,\n",
       "        4.98041098,  0.        ,  0.        ,  0.        ,  5.1171946 ,\n",
       "        5.31032527,  5.44169875,  5.57603568,  5.77670521,  6.00566826,\n",
       "        6.21881648,  6.34909701,  6.36346759,  6.22937252,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        5.39257273,  5.53300141,  5.60363525,  5.67867387,  5.88560806,\n",
       "        6.17630547,  6.49013316,  6.65740385,  6.70476559,  6.59780888,\n",
       "        0.        ,  0.        ,  0.        ,  5.33247197,  5.41894074,\n",
       "        5.55658835,  5.69093346,  5.77046297,  5.76903761,  5.71885932,\n",
       "        5.88216524,  6.22027578,  6.79084984,  6.97905232,  7.06225876,\n",
       "        7.05922212,  7.00087316,  6.92806144,  6.87982082,  5.40759326,\n",
       "        5.53197222,  5.72827233,  5.91820157,  6.02628036,  5.99260986,\n",
       "        0.        ,  0.        ,  0.        ,  7.10914736,  7.2955348 ,\n",
       "        7.36250968,  7.32845027,  7.22748359,  7.1134316 ,  7.04005962,\n",
       "        5.44708077,  5.61633911,  5.88361107,  6.17939148,  6.35064419,\n",
       "        6.36410586,  0.        ,  0.        ,  0.        ,  7.45648616,\n",
       "        7.63602416,  7.68665034,  7.61743178,  7.45958355,  7.28831012,\n",
       "        7.18002278,  5.44288549,  5.60362613,  5.92146065,  6.45658295,\n",
       "        6.700879  ,  6.94191523,  7.18260285,  7.42007997,  7.64761228,\n",
       "        7.85024108,  7.99633066,  8.03942386,  7.93689274,  7.69905672,\n",
       "        7.44618193,  7.28931188,  0.        ,  0.        ,  0.        ,\n",
       "        6.68513241,  6.90719575,  7.15262491,  7.40346798,  7.65487569,\n",
       "        7.90426162,  8.14466638,  8.3416401 ,  8.42866708,  8.31270954,\n",
       "        7.93399863,  7.57581906,  7.35333007,  0.        ,  0.        ,\n",
       "        0.        ,  6.8941983 ,  7.0773923 ,  7.32923773,  7.59727291,\n",
       "        7.87225141,  8.15276588,  8.43490547,  8.70436383,  8.88706575,\n",
       "        8.82446908,  8.03065382,  7.57684691,  7.35366926,  7.63452536,\n",
       "        7.4195919 ,  7.21456109,  7.12898529,  7.21020986,  7.45991347,\n",
       "        7.74631974,  8.04766574,  8.36618407,  8.70318989,  9.05488888,\n",
       "        9.40166934,  9.54104012,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  7.63452536,  7.31372915,  7.17821476,  7.26022203,\n",
       "        7.53024094,  7.82875752,  8.14843038,  8.49611716,  8.88350835,\n",
       "        9.33302403,  9.8939781 , 10.68610437, 11.80205199, 13.27563755,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pi_agent.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config=None\n",
    "config = default_pi_config.copy()\n",
    "if train_config is not None:\n",
    "    config.update(train_config)\n",
    "\n",
    "trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "old_policy_result = {\n",
    "    obs: -1 for obs in range(trainer.obs_dim)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 4.04.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 5.451.\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 7.716.\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 8.865.\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 12.471.\n",
      "[INFO]\tIn 5 iteration, current mean episode reward is 14.317.\n",
      "[INFO]\tIn 6 iteration, current mean episode reward is 14.72.\n",
      "[INFO]\tIn 7 iteration, current mean episode reward is 14.748.\n",
      "[INFO]\tIn 8 iteration, current mean episode reward is 14.867.\n",
      "[INFO]\tIn 9 iteration, current mean episode reward is 14.944.\n",
      "[INFO]\tIn 10 iteration, current mean episode reward is 14.958.\n",
      "[INFO]\tIn 11 iteration, current mean episode reward is 14.958.\n",
      "We found policy is not changed anymore at itertaion 12. Current mean episode reward is 14.958. Stop training.\n"
     ]
    }
   ],
   "source": [
    "for i in range(config['max_iteration']):\n",
    "\n",
    "    # train the agent\n",
    "    trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "    # [TODO] compare the new policy with old policy to check whether\n",
    "    #  should we stop. If new and old policy have same output given any\n",
    "    #  observation, them we consider the algorithm is converged and\n",
    "    #  should be stopped.\n",
    "    new_policy_result = {\n",
    "         obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "\n",
    "    should_stop = True if new_policy_result == old_policy_result else False\n",
    "    if should_stop:\n",
    "        print(\"We found policy is not changed anymore at \"\n",
    "              \"itertaion {}. Current mean episode reward \"\n",
    "              \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "        break\n",
    "    old_policy_result = new_policy_result\n",
    "\n",
    "    # evaluate the result\n",
    "    if i % config['evaluate_interval'] == 0:\n",
    "        print(\n",
    "            \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "            \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "#         if i > 20:\n",
    "#             print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "#                   \"({}) iterations to train a policy iteration \"\n",
    "#                   \"agent.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy_result = {\n",
    "         obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.11665608, 4.16635208, 4.25339101, 4.40424233, 4.68855554,\n",
       "       5.11494619, 5.13746829, 0.        , 4.19170723, 4.20556625,\n",
       "       4.23143033, 0.        , 0.        , 5.24742247, 5.31567124,\n",
       "       5.3368801 , 4.30486964, 4.32100258, 4.36355889, 0.        ,\n",
       "       0.        , 5.47066278, 5.52379209, 5.5198126 , 4.4191873 ,\n",
       "       4.42551869, 4.67047282, 5.36387035, 5.70859963, 5.80655143,\n",
       "       5.74828853, 5.68310017, 4.54677067, 0.        , 0.        ,\n",
       "       5.87508013, 6.12836497, 6.13872224, 5.92940455, 5.79012696,\n",
       "       4.81213497, 0.        , 0.        , 6.31103779, 6.55700059,\n",
       "       6.54441908, 6.03904336, 5.82630777, 5.22332155, 5.79279059,\n",
       "       6.26942875, 6.69227622, 7.01424187, 7.13585015, 0.        ,\n",
       "       0.        , 0.        , 6.06116059, 6.51320212, 6.95434526,\n",
       "       7.42715203, 8.06512739, 8.94750056, 0.        ])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_vi_config = dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1,  # don't need to update policy each iteration\n",
    "    gamma=1.0,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def value_iteration(train_config=None):\n",
    "    config = default_vi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # [TODO] initialize Value Iteration Trainer. Remember to pass\n",
    "    #  config['gamma'] to it.\n",
    "    trainer = ValueIterationTrainer(gamma=config['gamma'])\n",
    "\n",
    "    old_state_value_table = trainer.table.copy()\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "        new_state_value_table = trainer.table\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\"[INFO]\\tIn {} iteration, current \"\n",
    "                  \"mean episode reward is {}.\".format(\n",
    "                i, trainer.evaluate()\n",
    "            ))\n",
    "\n",
    "            # [TODO] compare the new policy with old policy to check should\n",
    "            #  we stop.\n",
    "            # [HINT] If new and old policy have same output given any\n",
    "            #  observation, them we consider the algorithm is converged and\n",
    "            #  should be stopped.\n",
    "\n",
    "            should_stop = True if np.sum(np.abs(old_state_value_table - new_state_value_table)) < config[\"eps\"] else False\n",
    "            \n",
    "            \n",
    "            if should_stop:\n",
    "                print(\"We found policy is not changed anymore at \"\n",
    "                      \"itertaion {}. Current mean episode reward \"\n",
    "                      \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "                break\n",
    "            old_state_value_table = new_state_value_table\n",
    "            if i > 3000:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(\n",
    "                    i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 3.598.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 3.384.\n",
      "We found policy is not changed anymore at itertaion 1. Current mean episode reward is 3.384. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
