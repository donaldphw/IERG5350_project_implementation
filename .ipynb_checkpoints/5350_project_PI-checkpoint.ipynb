{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque\n",
    "import csv\n",
    "from SailingEnv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 500,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map_name = \"8x8\",  \n",
    "    is_slippery = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingEnv(environment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation space: Discrete(64)\n",
      "Current action space: Discrete(4)\n",
      "0 in action space? True\n",
      "5 in action space? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation space: {}\".format(env.observation_space))\n",
    "print(\"Current action space: {}\".format(env.action_space))\n",
    "print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n",
    "print(\"5 in action space? {}\".format(env.action_space.contains(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     # take random action\n",
    "#     # [TODO] Uncomment next line\n",
    "#     obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "#     # render the environment\n",
    "#     env.render()  # [TODO] Uncomment this line\n",
    "\n",
    "#     print(\"Current step: {}\\nCurrent observation: {}\\nCurrent reward: {}\\n\"\n",
    "#           \"Whether we are done: {}\\ninfo: {}\".format(\n",
    "#         env.current_step, obs, reward, done, info\n",
    "#     ))\n",
    "#     wait(sleep=0.4)\n",
    "#     # [TODO] terminate the loop if done\n",
    "#     if done:\n",
    "#         break\n",
    "# #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='SailingEnv', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = SailingEnv(environment_config)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        ep_step = 0\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            ep_step += 1\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "        steps.append(ep_step)  \n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "        \n",
    "    return np.mean(rewards), np.mean(steps)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name=\"SailingEnv\", model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = SailingEnv(environment_config)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.P[state][act]\n",
    "#         print(transitions)\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('SailingEnv')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        mean_reward, mean_step = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return [mean_reward, mean_step]\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PolicyItertaionTrainer(TabularRLTrainerAbstract):\n",
    "    def random_policy(ops):\n",
    "            return np.random.choice(self.action_dim, size=(self.env.observation_space.n))\n",
    "    def __init__(self, gamma=1.0, eps=1e-10, env_name='SailingEnv'):\n",
    "        super(PolicyItertaionTrainer, self).__init__(env_name)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # value function convergence criterion\n",
    "        self.eps = eps\n",
    "\n",
    "        # build the value table for each possible observation\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        policy_array = np.random.randint(self.action_dim, size = (self.obs_dim))\n",
    "\n",
    "        self.policy = lambda obs: policy_array[obs]\n",
    "\n",
    "        test_random_policy(self.policy, self.env)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "        self.update_value_function()\n",
    "        self.update_policy()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        count = 0  # count the steps of value updates\n",
    "        while True:\n",
    "            old_table = self.table.copy()\n",
    "\n",
    "            for state in range(self.obs_dim):\n",
    "                \n",
    "                act = self.policy(state)\n",
    "                transition_list = self._get_transitions(state, act)\n",
    "                state_value = 0\n",
    "                for transition in transition_list:\n",
    "                    \n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    \n",
    "\n",
    "                    state_value += prob * (reward + self.gamma * old_table[next_state])\n",
    "                # update the state value\n",
    "                    \n",
    "                self.table[state] = state_value\n",
    "            \n",
    "\n",
    "            # [TODO] Compare the old_table and current table to\n",
    "            #  decide whether to break the value update process.\n",
    "            # hint: you should use self.eps, old_table and self.table\n",
    "            should_break = True if np.sum(np.abs(old_table - self.table)) < self.eps else False\n",
    "\n",
    "            if should_break:\n",
    "                break\n",
    "            count += 1\n",
    "            if count % 20000 == 0:\n",
    "                # disable this part if you think debug message annoying.\n",
    "                \n",
    "                print(\"[DEBUG]\\tUpdated values for {} steps. \"\n",
    "                      \"Difference between new and old table is: {}\".format(\n",
    "                    count, np.sum(np.abs(old_table - self.table))\n",
    "                ))\n",
    "#             if count > 4000:\n",
    "#                 print(\"[HINT] Are you sure your codes is OK? It shouldn't be \"\n",
    "#                       \"so hard to update the value function. You already \"\n",
    "#                       \"use {} steps to update value function within \"\n",
    "#                       \"single iteration.\".format(count))\n",
    "#             if count > 6000:\n",
    "#                 raise ValueError(\"Clearly your code has problem. Check it!\")\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"You need to define a new policy function, given current\n",
    "        value function. The best action for a given state is the one that\n",
    "        has greatest expected return.\n",
    "\n",
    "        To optimize computing efficiency, we introduce a policy table,\n",
    "        which take state as index and return the action given a state.\n",
    "        \"\"\"\n",
    "        policy_table = np.zeros([self.obs_dim, ], dtype=np.int)\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            \n",
    "            # [TODO] assign the action with greatest \"value\"\n",
    "            # to policy_table[state]\n",
    "            # hint: what is the proper \"value\" here?\n",
    "            #  you should use table, gamma, reward, prob,\n",
    "            #  next_state and self._get_transitions() function\n",
    "            #  as what we done at self.update_value_function()\n",
    "            #  Bellman equation may help.\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            best_action = np.argmax(state_action_values)\n",
    "            \n",
    "            \n",
    "            policy_table[state] = best_action\n",
    "        self.policy = lambda obs: policy_table[obs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "\n",
    "class ValueIterationTrainer(PolicyItertaionTrainer):\n",
    "    \"\"\"Note that we inherate Policy Iteration Trainer, to resue the\n",
    "    code of update_policy(). It's same since it get optimal policy from\n",
    "    current state-value table (self.table).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, env_name='SailingEnv'):\n",
    "        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "\n",
    "        self.update_value_function()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.table.copy()\n",
    "        for state in range(self.obs_dim):\n",
    "            state_value = 0\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "\n",
    "                state_value = np.max(state_action_values)\n",
    "                self.table[state] = state_value\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().evaluate()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_pi_config = dict(\n",
    "    max_iteration=20,\n",
    "    evaluate_interval=1,\n",
    "    gamma=0.99,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def policy_iteration(train_config=None):\n",
    "    config = default_pi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "        \n",
    "    trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "    old_policy_result = {\n",
    "        obs: -1 for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "    benchmark = []\n",
    "    for i in range(config['max_iteration']):\n",
    "\n",
    "        # train the agent\n",
    "        trainer.train()  \n",
    "        new_policy_result = {\n",
    "             obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "        }\n",
    "        \n",
    "#         should_stop = True if new_policy_result == old_policy_result else False\n",
    "#         if should_stop:\n",
    "#             print(\"We found policy is not changed anymore at \"\n",
    "#                   \"itertaion {}. Current mean episode reward \"\n",
    "#                   \"is {}. Current mean episode step is {}. Stop training.\".format(i, trainer.evaluate()[0], trainer.evaluate()[1]))\n",
    "#             break\n",
    "        old_policy_result = new_policy_result\n",
    "#         print(old_policy_result)\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            result = dict(iteration=i, mean_reward=trainer.evaluate()[0],mean_step=trainer.evaluate()[1])\n",
    "            benchmark.append(result)\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}, current mean episode step is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()[0], trainer.evaluate()[1]))\n",
    "\n",
    "#             if i > 20:\n",
    "#                 print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "#                       \"({}) iterations to train a policy iteration \"\n",
    "#                       \"agent.\".format(i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer, benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 16.398, current mean episode step is 9.602.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 348.332, current mean episode step is 56.068.\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 479.937, current mean episode step is 55.663.\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 710.843, current mean episode step is 80.957.\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 903.417, current mean episode step is 96.583.\n",
      "[INFO]\tIn 5 iteration, current mean episode reward is 904.794, current mean episode step is 95.206.\n",
      "[INFO]\tIn 6 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 7 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 8 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 9 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 10 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 11 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 12 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 13 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 14 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 15 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 16 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 17 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 18 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n",
      "[INFO]\tIn 19 iteration, current mean episode reward is 906.714, current mean episode step is 93.286.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# It may be confusing to call a trainer agent. But that's what we normally do.\n",
    "pi_agent, benchmark = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = \"benchmark/\"\n",
    "benchmark_col = [\"iteration\", \"mean_reward\", \"mean_step\"]\n",
    "benchmark_name = \"%sPI_%s_%s.csv\" % (benchmark_dir, environment_config[\"map_name\"], environment_config[\"total_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(benchmark_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=benchmark_col)\n",
    "        writer.writeheader()\n",
    "        for data in benchmark:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_vi_config = dict(\n",
    "    max_iteration=20,\n",
    "    evaluate_interval=1,  # don't need to update policy each iteration\n",
    "    gamma=0.99,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def value_iteration(train_config=None):\n",
    "    config = default_vi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # [TODO] initialize Value Iteration Trainer. Remember to pass\n",
    "    #  config['gamma'] to it.\n",
    "    trainer = ValueIterationTrainer(gamma=config['gamma'])\n",
    "\n",
    "    old_state_value_table = trainer.table.copy()\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train() \n",
    "        new_state_value_table = trainer.table\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}, current mean episode step is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()[0], trainer.evaluate()[1]))\n",
    "\n",
    "\n",
    "#             should_stop = True if np.sum(np.abs(old_state_value_table - new_state_value_table)) < config[\"eps\"] else False\n",
    "            \n",
    "            \n",
    "#             if should_stop:\n",
    "#                 print(\"We found policy is not changed anymore at \"\n",
    "#                   \"itertaion {}. Current mean episode reward \"\n",
    "#                   \"is {}. Current mean episode step is {}. Stop training.\".format(i, trainer.evaluate()[0], trainer.evaluate()[1]))\n",
    "#                 break\n",
    "            old_state_value_table = new_state_value_table\n",
    "            if i > 3000:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(\n",
    "                    i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 44.756, current mean episode step is 67.244.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is -64.627, current mean episode step is 64.627.\n",
      "We found policy is not changed anymore at itertaion 1. Current mean episode reward is -64.627. Current mean episode step is 64.627. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
