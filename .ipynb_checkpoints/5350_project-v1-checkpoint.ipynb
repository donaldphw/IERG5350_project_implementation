{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "#     \"4x4\": [\n",
    "#         \"SFFF\",\n",
    "#         \"FHFH\",\n",
    "#         \"FFFH\",\n",
    "#         \"HFFG\"\n",
    "#     ],\n",
    "    \"8x8\": [\n",
    "        \"SWWWWWWW\",\n",
    "        \"WWWOOWWW\",\n",
    "        \"WWWOOWWW\",\n",
    "        \"WWWWWWWW\",\n",
    "        \"WOOWWWWW\",\n",
    "        \"WOOWWWWW\",\n",
    "        \"WWWWWOWW\",\n",
    "        \"WWWWWWWD\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "def get_destination(MAP):\n",
    "            destination = []\n",
    "            row = len(MAP)\n",
    "            col = len(MAP[row-1])\n",
    "\n",
    "            for i in range(row):\n",
    "                for j in range(col):\n",
    "\n",
    "                    newletter = MAP[i][j]\n",
    "                    if newletter == \"D\":\n",
    "\n",
    "                        destination.append(i*col + j)\n",
    "            return destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SailingEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "#     def __init__(self, desc=None, map_name=\"8x8\", is_slippery=True):\n",
    "    def __init__(self, config):\n",
    "#         if desc is None and map_name is None:\n",
    "#             desc = generate_random_map()\n",
    "#         elif desc is None:\n",
    "#             desc = MAPS[map_name]\n",
    "        \n",
    "        desc = MAPS[config[\"map_name\"]]\n",
    "        is_slippery=True\n",
    "        self.current_step = 0\n",
    "        self.total_steps = config[\"total_steps\"] \n",
    "        self.destinations = get_destination(desc)\n",
    "        self.total_destinations = len(self.destinations)\n",
    "        self.destinations_dict = {D: False for D in self.destinations}\n",
    "        self.num_reached_destinations = 0\n",
    "        \n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "            \n",
    "        self.desc = desc = np.asarray(desc, dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, self.total_destinations)\n",
    "        \n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "        \n",
    "        \n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "        \n",
    "        \n",
    "        def seed(self, seed=None):\n",
    "            self.np_random, seed = seeding.np_random(seed)\n",
    "            return [seed]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        def update_reached_destinations(newstate):\n",
    "            if newstate in self.destinations_dict:\n",
    "                if self.destinations_dict[newstate] == False:\n",
    "                    self.destinations_dict[newstate] = True\n",
    "                    self.num_reached_destinations +=1\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            \n",
    "        def get_reward():\n",
    "            for key, value in self.destinations_dict.items():\n",
    "                if value == False:\n",
    "                    return float(0.0)\n",
    "            return float(1.0)\n",
    "            \n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            \n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            is_updated_destinations = update_reached_destinations(newstate)\n",
    "            \n",
    "                \n",
    "            done = bytes(newletter) in b'OD'\n",
    "#             done = self.current_step == self.total_steps\n",
    "\n",
    "        \n",
    "\n",
    "            if is_updated_destinations == True:\n",
    "                done =  self.num_reached_destinations == self.total_destinations\n",
    "\n",
    "#             reward = float(newletter == b'D')\n",
    "            reward = get_reward()\n",
    "#             reward = float(self.num_reached_destinations == self.total_destinations)\n",
    "            return newstate, reward, done\n",
    "        \n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'OD':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                                li.append((\n",
    "                                    1. / 3.,\n",
    "                                    *update_probability_matrix(row, col, b)\n",
    "                                ))\n",
    "                        else:\n",
    "                            li.append((\n",
    "                                1., *update_probability_matrix(row, col, a)\n",
    "                            ))\n",
    "\n",
    "        super(SailingEnv, self).__init__(nS, nA, P, isd)\n",
    "    \n",
    "    def update_reached_destination(self, newstate):\n",
    "        if newstate in self.destinations_dict:\n",
    "            if self.destinations_dict[newstate] == False:\n",
    "                self.destinations_dict[newstate] = True\n",
    "                self.num_reached_destinations +=1\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        \n",
    "#         is_updated_destinations = self.update_reached_destination(s)\n",
    "#         r = float(self.num_reached_destinations == self.total_destinations)\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        \n",
    "        \n",
    "#         if is_updated_destinations == True:\n",
    "#             d =  self.num_reached_destinations == self.total_destinations\n",
    "\n",
    "        if self.current_step == self.total_steps:\n",
    "            d =  True\n",
    "        if d != True:\n",
    "            self.current_step = self.current_step + 1\n",
    "\n",
    "            \n",
    "        return (int(s), r, d, {\"prob\": p})\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        self.num_reached_destinations = 0\n",
    "\n",
    "        self.destinations_dict = {D: False for D in self.destinations}\n",
    "        return int(self.s)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format(\n",
    "                [\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 100,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map_name = \"8x8\",   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingEnv(environment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation space: Discrete(64)\n",
      "Current action space: Discrete(4)\n",
      "0 in action space? True\n",
      "5 in action space? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation space: {}\".format(env.observation_space))\n",
    "print(\"Current action space: {}\".format(env.action_space))\n",
    "print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n",
    "print(\"5 in action space? {}\".format(env.action_space.contains(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SWWWWWWW\n",
      "WWWOOWWW\n",
      "WWWOOWWW\n",
      "WWWWWWWW\n",
      "WOOWWWWW\n",
      "W\u001b[41mO\u001b[0mOWWWWW\n",
      "WWWWWOWW\n",
      "WWWWWWWD\n",
      "Current step: 18\n",
      "Current observation: 41\n",
      "Current reward: 0.0\n",
      "Whether we are done: True\n",
      "info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # take random action\n",
    "    # [TODO] Uncomment next line\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "    # render the environment\n",
    "    env.render()  # [TODO] Uncomment this line\n",
    "\n",
    "    print(\"Current step: {}\\nCurrent observation: {}\\nCurrent reward: {}\\n\"\n",
    "          \"Whether we are done: {}\\ninfo: {}\".format(\n",
    "        env.current_step, obs, reward, done, info\n",
    "    ))\n",
    "    wait(sleep=0.4)\n",
    "    # [TODO] terminate the loop if done\n",
    "    if done:\n",
    "        break\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='SailingEnv', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = SailingEnv(environment_config)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name=\"SailingEnv\", model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = SailingEnv(environment_config)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.P[state][act]\n",
    "#         print(transitions)\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('SailingEnv')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PolicyItertaionTrainer(TabularRLTrainerAbstract):\n",
    "    def random_policy(ops):\n",
    "            return np.random.choice(self.action_dim, size=(self.env.observation_space.n))\n",
    "    def __init__(self, gamma=1.0, eps=1e-10, env_name='SailingEnv'):\n",
    "        super(PolicyItertaionTrainer, self).__init__(env_name)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # value function convergence criterion\n",
    "        self.eps = eps\n",
    "\n",
    "        # build the value table for each possible observation\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # [TODO] you need to implement a random policy at the beginning.\n",
    "        # It is a function that take an integer (state or say observation)\n",
    "        # as input and return an interger (action).\n",
    "        # remember, you can use self.action_dim to get the dimension (range)\n",
    "        # of the action, which is an integer in range\n",
    "        # [0, ..., self.action_dim - 1]\n",
    "        # hint: generating random action at each call of policy may lead to\n",
    "        #  failure of convergence, try generate random actions at initializtion\n",
    "        #  and fix it during the training.\n",
    "        policy_array = np.random.randint(self.action_dim, size = (self.obs_dim))\n",
    "#         def random_policy(state):\n",
    "#             return random_policy_array[state]\n",
    "        \n",
    "#         self.policy = random_policy\n",
    "        self.policy = lambda obs: policy_array[obs]\n",
    "        # test your random policy\n",
    "        test_random_policy(self.policy, self.env)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        # hint: the value function is equivalent to self.table,\n",
    "        #  a numpy array with length 64.\n",
    "\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "        self.update_value_function()\n",
    "        self.update_policy()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        count = 0  # count the steps of value updates\n",
    "        while True:\n",
    "            old_table = self.table.copy()\n",
    "\n",
    "            for state in range(self.obs_dim):\n",
    "                \n",
    "                act = self.policy(state)\n",
    "                transition_list = self._get_transitions(state, act)\n",
    "                state_value = 0\n",
    "                for transition in transition_list:\n",
    "                    \n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    \n",
    "                    # [TODO] what is the right state value?\n",
    "                    # hint: you should use reward, self.gamma, old_table, prob,\n",
    "                    # and next_state to compute the state value\n",
    "#                     pass\n",
    "                    state_value += prob * (reward + self.gamma * old_table[next_state])\n",
    "                # update the state value\n",
    "                    \n",
    "                self.table[state] = state_value\n",
    "            \n",
    "\n",
    "            # [TODO] Compare the old_table and current table to\n",
    "            #  decide whether to break the value update process.\n",
    "            # hint: you should use self.eps, old_table and self.table\n",
    "            should_break = True if np.sum(np.abs(old_table - self.table)) < self.eps else False\n",
    "\n",
    "            if should_break:\n",
    "                break\n",
    "            count += 1\n",
    "            if count % 20000 == 0:\n",
    "                # disable this part if you think debug message annoying.\n",
    "                \n",
    "                print(\"[DEBUG]\\tUpdated values for {} steps. \"\n",
    "                      \"Difference between new and old table is: {}\".format(\n",
    "                    count, np.sum(np.abs(old_table - self.table))\n",
    "                ))\n",
    "#             if count > 4000:\n",
    "#                 print(\"[HINT] Are you sure your codes is OK? It shouldn't be \"\n",
    "#                       \"so hard to update the value function. You already \"\n",
    "#                       \"use {} steps to update value function within \"\n",
    "#                       \"single iteration.\".format(count))\n",
    "#             if count > 6000:\n",
    "#                 raise ValueError(\"Clearly your code has problem. Check it!\")\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"You need to define a new policy function, given current\n",
    "        value function. The best action for a given state is the one that\n",
    "        has greatest expected return.\n",
    "\n",
    "        To optimize computing efficiency, we introduce a policy table,\n",
    "        which take state as index and return the action given a state.\n",
    "        \"\"\"\n",
    "        policy_table = np.zeros([self.obs_dim, ], dtype=np.int)\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            \n",
    "            # [TODO] assign the action with greatest \"value\"\n",
    "            # to policy_table[state]\n",
    "            # hint: what is the proper \"value\" here?\n",
    "            #  you should use table, gamma, reward, prob,\n",
    "            #  next_state and self._get_transitions() function\n",
    "            #  as what we done at self.update_value_function()\n",
    "            #  Bellman equation may help.\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            best_action = np.argmax(state_action_values)\n",
    "            \n",
    "            \n",
    "            policy_table[state] = best_action\n",
    "        self.policy = lambda obs: policy_table[obs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "\n",
    "class ValueIterationTrainer(PolicyItertaionTrainer):\n",
    "    \"\"\"Note that we inherate Policy Iteration Trainer, to resue the\n",
    "    code of update_policy(). It's same since it get optimal policy from\n",
    "    current state-value table (self.table).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, env_name='SailingEnv'):\n",
    "        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "#         self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # In value iteration, we do not explicit require a\n",
    "        # policy instance to run. We update value function\n",
    "        # directly based on the transitions. Therefore, we\n",
    "        # don't need to run self.update_policy() in each step.\n",
    "        self.update_value_function()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.table.copy()\n",
    "        for state in range(self.obs_dim):\n",
    "            state_value = 0\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            # [TODO] what should be de right state value?\n",
    "            # hint: try to compute the state_action_values first\n",
    "                state_value = np.max(state_action_values)\n",
    "#                 print(state_value)\n",
    "                self.table[state] = state_value\n",
    "\n",
    "\n",
    "        # Till now the one step value update is finished.\n",
    "        # You can see that we do not use a inner loop to update\n",
    "        # the value function like what we did in policy iteration.\n",
    "        # This is because to compute the state value, which is\n",
    "        # a expectation among all possible action given by a\n",
    "        # specified policy, we **pretend** already own the optimal\n",
    "        # policy (the max operation).\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().evaluate()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_pi_config = dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=1,\n",
    "    gamma=0.99,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def policy_iteration(train_config=None):\n",
    "    config = default_pi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "        \n",
    "    trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "    old_policy_result = {\n",
    "        obs: -1 for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # [TODO] compare the new policy with old policy to check whether\n",
    "        #  should we stop. If new and old policy have same output given any\n",
    "        #  observation, them we consider the algorithm is converged and\n",
    "        #  should be stopped.\n",
    "        new_policy_result = {\n",
    "             obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "        }\n",
    "        \n",
    "        should_stop = True if new_policy_result == old_policy_result else False\n",
    "        if should_stop:\n",
    "            print(\"We found policy is not changed anymore at \"\n",
    "                  \"itertaion {}. Current mean episode reward \"\n",
    "                  \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "            break\n",
    "        old_policy_result = new_policy_result\n",
    "#         print(old_policy_result)\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "#             if i > 20:\n",
    "#                 print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "#                       \"({}) iterations to train a policy iteration \"\n",
    "#                       \"agent.\".format(i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 38.951.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 40.171.\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 47.483.\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 46.626.\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 46.626.\n",
      "We found policy is not changed anymore at itertaion 5. Current mean episode reward is 46.626. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# It may be confusing to call a trainer agent. But that's what we normally do.\n",
    "pi_agent = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----State Value Mapping-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\n",
      "|-----+-----+-----+-----+-----+-----+-----+-----+-----|\n",
      "| 0   |52.077|51.702|51.074|50.630|51.719|54.376|54.811|54.959|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 1   |53.655|52.895|51.993|0.000|0.000|55.590|56.323|56.625|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 2   |56.041|54.639|53.587|0.000|0.000|57.755|58.461|58.642|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 3   |59.141|55.945|55.752|59.413|60.901|60.964|60.758|60.600|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 4   |64.033|0.000|0.000|63.386|64.172|63.081|62.549|62.280|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 5   |70.865|0.000|0.000|68.494|67.993|64.435|64.182|63.897|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 6   |79.845|79.286|78.115|76.179|73.112|0.000|66.158|65.550|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 7   |82.823|82.302|81.245|79.620|77.377|74.449|70.747|0.000|\n",
      "|     |     |     |     |     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pi_agent.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config=None\n",
    "config = default_pi_config.copy()\n",
    "if train_config is not None:\n",
    "    config.update(train_config)\n",
    "\n",
    "trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "old_policy_result = {\n",
    "    obs: -1 for obs in range(trainer.obs_dim)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 10.503.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 10.503.\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 12.514.\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 13.365.\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 13.365.\n",
      "We found policy is not changed anymore at itertaion 5. Current mean episode reward is 13.365. Stop training.\n"
     ]
    }
   ],
   "source": [
    "for i in range(config['max_iteration']):\n",
    "\n",
    "    # train the agent\n",
    "    trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "    # [TODO] compare the new policy with old policy to check whether\n",
    "    #  should we stop. If new and old policy have same output given any\n",
    "    #  observation, them we consider the algorithm is converged and\n",
    "    #  should be stopped.\n",
    "    new_policy_result = {\n",
    "         obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "\n",
    "    should_stop = True if new_policy_result == old_policy_result else False\n",
    "    if should_stop:\n",
    "        print(\"We found policy is not changed anymore at \"\n",
    "              \"itertaion {}. Current mean episode reward \"\n",
    "              \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "        break\n",
    "    old_policy_result = new_policy_result\n",
    "\n",
    "    # evaluate the result\n",
    "    if i % config['evaluate_interval'] == 0:\n",
    "        print(\n",
    "            \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "            \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "#         if i > 20:\n",
    "#             print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "#                   \"({}) iterations to train a policy iteration \"\n",
    "#                   \"agent.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy_result = {\n",
    "         obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15078.51660623, 15079.46358143, 15081.35753183, 15084.19845742,\n",
       "       15087.98635821, 15092.7212342 , 15092.24988607, 15092.31351583,\n",
       "       15078.71847029, 15080.18296794, 15083.6294857 ,     0.        ,\n",
       "           0.        , 15093.50824073, 15093.13323151, 15093.26049103,\n",
       "       15080.63221379, 15083.49293249, 15090.61059093,     0.        ,\n",
       "           0.        , 15095.93289007, 15094.83629261, 15095.15444143,\n",
       "       15129.9169692 , 15106.03975209, 15105.65632979, 15121.26562156,\n",
       "       15115.73922925, 15100.40111206, 15097.16818011, 15096.41956544,\n",
       "       15180.14869981,     0.        ,     0.        , 15143.34828084,\n",
       "       15126.49792931, 15109.04924121, 15101.19565746, 15098.63166465,\n",
       "       15231.32740561,     0.        ,     0.        , 15183.22826683,\n",
       "       15155.65229268, 15124.16288718, 15108.73410283, 15101.79073905,\n",
       "       15283.45308662, 15279.34703701, 15270.1084254 , 15251.63120218,\n",
       "       15217.24303675,     0.        ,     0.        , 15105.89678866,\n",
       "       15285.50611142, 15282.42657422, 15277.29401221, 15265.4891196 ,\n",
       "       15242.39259057,     0.        ,     0.        , 15107.94981346])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_vi_config = dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1,  # don't need to update policy each iteration\n",
    "    gamma=1.0,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def value_iteration(train_config=None):\n",
    "    config = default_vi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # [TODO] initialize Value Iteration Trainer. Remember to pass\n",
    "    #  config['gamma'] to it.\n",
    "    trainer = ValueIterationTrainer(gamma=config['gamma'])\n",
    "\n",
    "    old_state_value_table = trainer.table.copy()\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "        new_state_value_table = trainer.table\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\"[INFO]\\tIn {} iteration, current \"\n",
    "                  \"mean episode reward is {}.\".format(\n",
    "                i, trainer.evaluate()\n",
    "            ))\n",
    "\n",
    "            # [TODO] compare the new policy with old policy to check should\n",
    "            #  we stop.\n",
    "            # [HINT] If new and old policy have same output given any\n",
    "            #  observation, them we consider the algorithm is converged and\n",
    "            #  should be stopped.\n",
    "\n",
    "            should_stop = True if np.sum(np.abs(old_state_value_table - new_state_value_table)) < config[\"eps\"] else False\n",
    "            \n",
    "            \n",
    "            if should_stop:\n",
    "                print(\"We found policy is not changed anymore at \"\n",
    "                      \"itertaion {}. Current mean episode reward \"\n",
    "                      \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "                break\n",
    "            old_state_value_table = new_state_value_table\n",
    "            if i > 3000:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(\n",
    "                    i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n",
      "We found policy is not changed anymore at itertaion 1. Current mean episode reward is 0.0. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90.91559912,  90.80966485,  90.42517183,  71.36716655,\n",
       "        70.99972671,  89.53100919,  92.89660844,  94.02963571,\n",
       "        93.5284845 ,  93.12533355,  92.46179045,   0.        ,\n",
       "         0.        ,  93.14913383,  94.96936586,  95.88638258,\n",
       "        96.90659184,  95.80716524,  95.04515151,   0.        ,\n",
       "         0.        ,  97.31544826,  98.24565007,  98.4550588 ,\n",
       "       100.99906101,  97.89090837,  98.07593581, 102.49887924,\n",
       "       103.91103029, 102.93881759, 101.34569766, 100.49366978,\n",
       "       106.77805019,   0.        ,   0.        , 107.92153262,\n",
       "       108.69994822, 105.95156705, 102.98124328, 101.3334267 ,\n",
       "       114.98521913,   0.        ,   0.        , 114.9822795 ,\n",
       "       114.63646928, 107.41150049, 102.8233863 ,  99.22296439,\n",
       "       125.62260708, 126.74067727, 126.48425193, 124.81243779,\n",
       "       121.43955746,   0.        ,  72.0965644 ,  77.25312391,\n",
       "       129.87451641, 130.54550157, 130.32860892, 128.94022785,\n",
       "       126.08318354,   0.        ,  37.49701523,   0.        ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_agent.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)]},\n",
       " 5: {0: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 6: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)]},\n",
       " 7: {0: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)]},\n",
       " 8: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False)]},\n",
       " 14: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)]},\n",
       " 16: {0: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)]},\n",
       " 17: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)]},\n",
       " 18: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 19, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)]},\n",
       " 19: {0: [(1.0, 19, 0, True)],\n",
       "  1: [(1.0, 19, 0, True)],\n",
       "  2: [(1.0, 19, 0, True)],\n",
       "  3: [(1.0, 19, 0, True)]},\n",
       " 20: {0: [(1.0, 20, 0, True)],\n",
       "  1: [(1.0, 20, 0, True)],\n",
       "  2: [(1.0, 20, 0, True)],\n",
       "  3: [(1.0, 20, 0, True)]},\n",
       " 21: {0: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False)]},\n",
       " 22: {0: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)]},\n",
       " 23: {0: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)]},\n",
       " 24: {0: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)]},\n",
       " 25: {0: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)]},\n",
       " 26: {0: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)]},\n",
       " 27: {0: [(0.3333333333333333, 19, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False)]},\n",
       " 28: {0: [(0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False)]},\n",
       " 29: {0: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False)]},\n",
       " 30: {0: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False)]},\n",
       " 31: {0: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)]},\n",
       " 32: {0: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)]},\n",
       " 33: {0: [(1.0, 33, 0, True)],\n",
       "  1: [(1.0, 33, 0, True)],\n",
       "  2: [(1.0, 33, 0, True)],\n",
       "  3: [(1.0, 33, 0, True)]},\n",
       " 34: {0: [(1.0, 34, 0, True)],\n",
       "  1: [(1.0, 34, 0, True)],\n",
       "  2: [(1.0, 34, 0, True)],\n",
       "  3: [(1.0, 34, 0, True)]},\n",
       " 35: {0: [(0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False)]},\n",
       " 36: {0: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False)]},\n",
       " 37: {0: [(0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)]},\n",
       " 38: {0: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)]},\n",
       " 39: {0: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)]},\n",
       " 40: {0: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 41, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False)]},\n",
       " 41: {0: [(1.0, 41, 0, True)],\n",
       "  1: [(1.0, 41, 0, True)],\n",
       "  2: [(1.0, 41, 0, True)],\n",
       "  3: [(1.0, 41, 0, True)]},\n",
       " 42: {0: [(1.0, 42, 0, True)],\n",
       "  1: [(1.0, 42, 0, True)],\n",
       "  2: [(1.0, 42, 0, True)],\n",
       "  3: [(1.0, 42, 0, True)]},\n",
       " 43: {0: [(0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 42, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, False)]},\n",
       " 44: {0: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 52, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False)]},\n",
       " 45: {0: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)]},\n",
       " 46: {0: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 54, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False)]},\n",
       " 47: {0: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 55, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False)]},\n",
       " 48: {0: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 49, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)]},\n",
       " 49: {0: [(0.3333333333333333, 41, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)]},\n",
       " 50: {0: [(0.3333333333333333, 42, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 49, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, False)]},\n",
       " 51: {0: [(0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 59, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 52, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False)]},\n",
       " 52: {0: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 60, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False)]},\n",
       " 53: {0: [(1.0, 53, 0, True)],\n",
       "  1: [(1.0, 53, 0, True)],\n",
       "  2: [(1.0, 53, 0, True)],\n",
       "  3: [(1.0, 53, 0, True)]},\n",
       " 54: {0: [(0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 55, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False)]},\n",
       " 55: {0: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, True)],\n",
       "  1: [(0.3333333333333333, 54, 1.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, False),\n",
       "   (0.3333333333333333, 55, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 63, 1.0, False),\n",
       "   (0.3333333333333333, 55, 1.0, False),\n",
       "   (0.3333333333333333, 47, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 55, 1.0, False),\n",
       "   (0.3333333333333333, 47, 1.0, False),\n",
       "   (0.3333333333333333, 54, 1.0, False)]},\n",
       " 56: {0: [(0.3333333333333333, 48, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 48, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 48, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False)]},\n",
       " 57: {0: [(0.3333333333333333, 49, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 56, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 49, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 49, 1.0, False),\n",
       "   (0.3333333333333333, 56, 1.0, False)]},\n",
       " 58: {0: [(0.3333333333333333, 50, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 57, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 50, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 50, 1.0, False),\n",
       "   (0.3333333333333333, 57, 1.0, False)]},\n",
       " 59: {0: [(0.3333333333333333, 51, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 58, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 60, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 60, 1.0, False),\n",
       "   (0.3333333333333333, 51, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 60, 1.0, False),\n",
       "   (0.3333333333333333, 51, 1.0, False),\n",
       "   (0.3333333333333333, 58, 1.0, False)]},\n",
       " 60: {0: [(0.3333333333333333, 52, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 60, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 59, 1.0, False),\n",
       "   (0.3333333333333333, 60, 1.0, False),\n",
       "   (0.3333333333333333, 61, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 60, 1.0, False),\n",
       "   (0.3333333333333333, 61, 1.0, False),\n",
       "   (0.3333333333333333, 52, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 61, 1.0, False),\n",
       "   (0.3333333333333333, 52, 1.0, False),\n",
       "   (0.3333333333333333, 59, 1.0, False)]},\n",
       " 61: {0: [(1.0, 61, 0, True)],\n",
       "  1: [(1.0, 61, 0, True)],\n",
       "  2: [(1.0, 61, 0, True)],\n",
       "  3: [(1.0, 61, 0, True)]},\n",
       " 62: {0: [(0.3333333333333333, 54, 1.0, False),\n",
       "   (0.3333333333333333, 61, 1.0, False),\n",
       "   (0.3333333333333333, 62, 1.0, False)],\n",
       "  1: [(0.3333333333333333, 61, 1.0, False),\n",
       "   (0.3333333333333333, 62, 1.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, False)],\n",
       "  2: [(0.3333333333333333, 62, 1.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, False),\n",
       "   (0.3333333333333333, 54, 1.0, False)],\n",
       "  3: [(0.3333333333333333, 63, 1.0, False),\n",
       "   (0.3333333333333333, 54, 1.0, False),\n",
       "   (0.3333333333333333, 61, 1.0, False)]},\n",
       " 63: {0: [(1.0, 63, 0, True)],\n",
       "  1: [(1.0, 63, 0, True)],\n",
       "  2: [(1.0, 63, 0, True)],\n",
       "  3: [(1.0, 63, 0, True)]}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
