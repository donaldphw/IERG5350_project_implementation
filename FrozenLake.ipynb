{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] != 'H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"8x8\", is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc, dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            done = bytes(newletter) in b'GH'\n",
    "            reward = float(newletter == b'G')\n",
    "            return newstate, reward, done\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                                li.append((\n",
    "                                    1. / 3.,\n",
    "                                    *update_probability_matrix(row, col, b)\n",
    "                                ))\n",
    "                        else:\n",
    "                            li.append((\n",
    "                                1., *update_probability_matrix(row, col, a)\n",
    "                            ))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format(\n",
    "                [\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation space: Discrete(64)\n",
      "Current action space: Discrete(4)\n",
      "0 in action space? True\n",
      "5 in action space? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation space: {}\".format(env.observation_space))\n",
    "print(\"Current action space: {}\".format(env.action_space))\n",
    "print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n",
    "print(\"5 in action space? {}\".format(env.action_space.contains(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFF\u001b[41mH\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Current observation: 29\n",
      "Current reward: 0.0\n",
      "Whether we are done: True\n",
      "info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # take random action\n",
    "    # [TODO] Uncomment next line\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "    # render the environment\n",
    "    env.render()  # [TODO] Uncomment this line\n",
    "\n",
    "    print(\"Current observation: {}\\nCurrent reward: {}\\n\"\n",
    "          \"Whether we are done: {}\\ninfo: {}\".format(\n",
    "     obs, reward, done, info\n",
    "    ))\n",
    "    wait(sleep=0.4)\n",
    "    # [TODO] terminate the loop if done\n",
    "    if done:\n",
    "        break\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='FrozenLake', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = FrozenLakeEnv()\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        \n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name=\"FrozenLake\", model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = FrozenLakeEnv()\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.P[state][act]\n",
    "#         print(transitions)\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('FrozenLake')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PolicyItertaionTrainer(TabularRLTrainerAbstract):\n",
    "    def random_policy(ops):\n",
    "            return np.random.choice(self.action_dim, size=(self.env.observation_space.n))\n",
    "    def __init__(self, gamma=1.0, eps=1e-10, env_name='FrozenLake'):\n",
    "        super(PolicyItertaionTrainer, self).__init__(env_name)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # value function convergence criterion\n",
    "        self.eps = eps\n",
    "\n",
    "        # build the value table for each possible observation\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # [TODO] you need to implement a random policy at the beginning.\n",
    "        # It is a function that take an integer (state or say observation)\n",
    "        # as input and return an interger (action).\n",
    "        # remember, you can use self.action_dim to get the dimension (range)\n",
    "        # of the action, which is an integer in range\n",
    "        # [0, ..., self.action_dim - 1]\n",
    "        # hint: generating random action at each call of policy may lead to\n",
    "        #  failure of convergence, try generate random actions at initializtion\n",
    "        #  and fix it during the training.\n",
    "        policy_array = np.random.randint(self.action_dim, size = (self.obs_dim))\n",
    "#         def random_policy(state):\n",
    "#             return random_policy_array[state]\n",
    "        \n",
    "#         self.policy = random_policy\n",
    "        self.policy = lambda obs: policy_array[obs]\n",
    "        # test your random policy\n",
    "        test_random_policy(self.policy, self.env)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        # hint: the value function is equivalent to self.table,\n",
    "        #  a numpy array with length 64.\n",
    "\n",
    "        self.table = np.zeros((self.obs_dim,))\n",
    "        self.update_value_function()\n",
    "        self.update_policy()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        count = 0  # count the steps of value updates\n",
    "        while True:\n",
    "            old_table = self.table.copy()\n",
    "\n",
    "            for state in range(self.obs_dim):\n",
    "                \n",
    "                act = self.policy(state)\n",
    "                transition_list = self._get_transitions(state, act)\n",
    "                state_value = 0\n",
    "                for transition in transition_list:\n",
    "                    \n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    \n",
    "                    # [TODO] what is the right state value?\n",
    "                    # hint: you should use reward, self.gamma, old_table, prob,\n",
    "                    # and next_state to compute the state value\n",
    "#                     pass\n",
    "                    state_value += prob * (reward + self.gamma * old_table[next_state])\n",
    "                # update the state value\n",
    "                    \n",
    "                self.table[state] = state_value\n",
    "            \n",
    "\n",
    "            # [TODO] Compare the old_table and current table to\n",
    "            #  decide whether to break the value update process.\n",
    "            # hint: you should use self.eps, old_table and self.table\n",
    "            should_break = True if np.sum(np.abs(old_table - self.table)) < self.eps else False\n",
    "\n",
    "            if should_break:\n",
    "                break\n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                # disable this part if you think debug message annoying.\n",
    "                \n",
    "                print(\"[DEBUG]\\tUpdated values for {} steps. \"\n",
    "                      \"Difference between new and old table is: {}\".format(\n",
    "                    count, np.sum(np.abs(old_table - self.table))\n",
    "                ))\n",
    "#             if count > 4000:\n",
    "#                 print(\"[HINT] Are you sure your codes is OK? It shouldn't be \"\n",
    "#                       \"so hard to update the value function. You already \"\n",
    "#                       \"use {} steps to update value function within \"\n",
    "#                       \"single iteration.\".format(count))\n",
    "#             if count > 6000:\n",
    "#                 raise ValueError(\"Clearly your code has problem. Check it!\")\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"You need to define a new policy function, given current\n",
    "        value function. The best action for a given state is the one that\n",
    "        has greatest expected return.\n",
    "\n",
    "        To optimize computing efficiency, we introduce a policy table,\n",
    "        which take state as index and return the action given a state.\n",
    "        \"\"\"\n",
    "        policy_table = np.zeros([self.obs_dim, ], dtype=np.int)\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            \n",
    "            # [TODO] assign the action with greatest \"value\"\n",
    "            # to policy_table[state]\n",
    "            # hint: what is the proper \"value\" here?\n",
    "            #  you should use table, gamma, reward, prob,\n",
    "            #  next_state and self._get_transitions() function\n",
    "            #  as what we done at self.update_value_function()\n",
    "            #  Bellman equation may help.\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            best_action = np.argmax(state_action_values)\n",
    "            \n",
    "            \n",
    "            policy_table[state] = best_action\n",
    "        self.policy = lambda obs: policy_table[obs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "\n",
    "class ValueIterationTrainer(PolicyItertaionTrainer):\n",
    "    \"\"\"Note that we inherate Policy Iteration Trainer, to resue the\n",
    "    code of update_policy(). It's same since it get optimal policy from\n",
    "    current state-value table (self.table).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, env_name='FrozenLake'):\n",
    "        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] value function may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "#         self.table = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # In value iteration, we do not explicit require a\n",
    "        # policy instance to run. We update value function\n",
    "        # directly based on the transitions. Therefore, we\n",
    "        # don't need to run self.update_policy() in each step.\n",
    "        self.update_value_function()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.table.copy()\n",
    "        for state in range(self.obs_dim):\n",
    "            state_value = 0\n",
    "            state_action_values = [0] * self.action_dim\n",
    "            for action in range(self.action_dim):\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "                    state_action_values[action] += prob * (reward + self.gamma * self.table[next_state])\n",
    "            # [TODO] what should be de right state value?\n",
    "            # hint: try to compute the state_action_values first\n",
    "                state_value = np.max(state_action_values)\n",
    "#                 print(state_value)\n",
    "                self.table[state] = state_value\n",
    "\n",
    "\n",
    "        # Till now the one step value update is finished.\n",
    "        # You can see that we do not use a inner loop to update\n",
    "        # the value function like what we did in policy iteration.\n",
    "        # This is because to compute the state value, which is\n",
    "        # a expectation among all possible action given by a\n",
    "        # specified policy, we **pretend** already own the optimal\n",
    "        # policy (the max operation).\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().evaluate()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Since in value itertaion we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_pi_config = dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=1,\n",
    "    gamma=1,\n",
    "    eps=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "def policy_iteration(train_config=None):\n",
    "    config = default_pi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "        \n",
    "    trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "    old_policy_result = {\n",
    "        obs: -1 for obs in range(trainer.obs_dim)\n",
    "    }\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # [TODO] compare the new policy with old policy to check whether\n",
    "        #  should we stop. If new and old policy have same output given any\n",
    "        #  observation, them we consider the algorithm is converged and\n",
    "        #  should be stopped.\n",
    "        new_policy_result = {\n",
    "             obs: trainer.table[obs] for obs in range(trainer.obs_dim)\n",
    "        }\n",
    "        \n",
    "        should_stop = True if new_policy_result == old_policy_result else False\n",
    "        if should_stop:\n",
    "            print(\"We found policy is not changed anymore at \"\n",
    "                  \"itertaion {}. Current mean episode reward \"\n",
    "                  \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "            break\n",
    "        old_policy_result = new_policy_result\n",
    "#         print(old_policy_result)\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "            if i > 20:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(i))\n",
    "\n",
    "    assert trainer.evaluate() > 0.8, \\\n",
    "        \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 1.8745861117128366e-06\n",
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.054.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.003287592360011595\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.0003243138393842458\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 3.614793910336825e-05\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 4.15499006039665e-06\n",
      "[INFO]\tIn 1 iteration, current mean episode reward is 0.535.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.007641424313319101\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 5.0042713591554044e-05\n",
      "[INFO]\tIn 2 iteration, current mean episode reward is 0.981.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.05488885712633666\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.002943718335260692\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.00015639474280071397\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 8.30865964038252e-06\n",
      "[INFO]\tIn 3 iteration, current mean episode reward is 0.994.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.08057430483958863\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.006249261521376101\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0003718111325436424\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 2.0554749065523903e-05\n",
      "[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 1.108144320166593e-06\n",
      "[INFO]\tIn 4 iteration, current mean episode reward is 1.0.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.09552655020323203\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.011594909585231433\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.00097089273463144\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 7.047335741627858e-05\n",
      "[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 4.747901510088193e-06\n",
      "[INFO]\tIn 5 iteration, current mean episode reward is 1.0.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.07182902656505606\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.004968687572845798\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0002869886891957302\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.5706847914770394e-05\n",
      "[INFO]\tIn 6 iteration, current mean episode reward is 1.0.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.07154226746958085\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.004937646720823655\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.00028502578670354384\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.5596292745820306e-05\n",
      "[INFO]\tIn 7 iteration, current mean episode reward is 1.0.\n",
      "[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.07154226746958085\n",
      "[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.004937646720823655\n",
      "[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.00028502578670354384\n",
      "[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.5596292745820306e-05\n",
      "We found policy is not changed anymore at itertaion 8. Current mean episode reward is 1.0. Stop training.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# It may be confusing to call a trainer agent. But that's what we normally do.\n",
    "pi_agent = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999891, 0.99999896, 0.99999903, 0.9999991 , 0.99999918,\n",
       "       0.99999925, 0.99999932, 0.99999936, 0.9999989 , 0.99999894,\n",
       "       0.999999  , 0.99999907, 0.99999914, 0.99999922, 0.9999993 ,\n",
       "       0.99999939, 0.99999786, 0.97819966, 0.92642883, 0.        ,\n",
       "       0.85661686, 0.94623086, 0.98207652, 0.99999945, 0.99999692,\n",
       "       0.93460226, 0.80108791, 0.47490283, 0.62362062, 0.        ,\n",
       "       0.94467706, 0.99999952, 0.9999961 , 0.82561032, 0.54223272,\n",
       "       0.        , 0.5393422 , 0.61118877, 0.85195514, 0.99999962,\n",
       "       0.99999546, 0.        , 0.        , 0.16804051, 0.38321723,\n",
       "       0.44226899, 0.        , 0.99999974, 0.99999502, 0.        ,\n",
       "       0.19467246, 0.12090432, 0.        , 0.33240099, 0.        ,\n",
       "       0.99999987, 0.99999479, 0.73155386, 0.4631131 , 0.        ,\n",
       "       0.27746699, 0.55493399, 0.77746699, 0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_agent.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_vi_config = dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1,  # don't need to update policy each iteration\n",
    "    gamma=1.0,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def value_iteration(train_config=None):\n",
    "    config = default_vi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # [TODO] initialize Value Iteration Trainer. Remember to pass\n",
    "    #  config['gamma'] to it.\n",
    "    trainer = ValueIterationTrainer(gamma=config['gamma'])\n",
    "\n",
    "    old_state_value_table = trainer.table.copy()\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "        new_state_value_table = trainer.table\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\"[INFO]\\tIn {} iteration, current \"\n",
    "                  \"mean episode reward is {}.\".format(\n",
    "                i, trainer.evaluate()\n",
    "            ))\n",
    "\n",
    "            # [TODO] compare the new policy with old policy to check should\n",
    "            #  we stop.\n",
    "            # [HINT] If new and old policy have same output given any\n",
    "            #  observation, them we consider the algorithm is converged and\n",
    "            #  should be stopped.\n",
    "\n",
    "            should_stop = True if np.sum(np.abs(old_state_value_table - new_state_value_table)) < config[\"eps\"] else False\n",
    "            \n",
    "            \n",
    "            if should_stop:\n",
    "                print(\"We found policy is not changed anymore at \"\n",
    "                      \"itertaion {}. Current mean episode reward \"\n",
    "                      \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "                break\n",
    "            old_state_value_table = new_state_value_table\n",
    "            if i > 3000:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(\n",
    "                    i))\n",
    "\n",
    "#     assert trainer.evaluate() > 0.8, \\\n",
    "#         \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "#         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-83a0d7d90cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell without modification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvi_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-2e06dff88c82>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(train_config)\u001b[0m\n\u001b[1;32m     29\u001b[0m             print(\"[INFO]\\tIn {} iteration, current \"\n\u001b[1;32m     30\u001b[0m                   \"mean episode reward is {}.\".format(\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             ))\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-914317c7095b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         so we need to retrieve it when we need it.\"\"\"\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d2a097443cda>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"Use the function you write to evaluate current policy.\n\u001b[1;32m     62\u001b[0m         Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cc30cf1da5dc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(policy, num_episodes, seed, env_name, render)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# [TODO] run the environment and terminate it if done, collect the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# reward at each step and sum them to the episode reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m     \"\"\"\n\u001b[0;32m-> 2470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)]},\n",
       " 5: {0: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 6: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)]},\n",
       " 7: {0: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)]},\n",
       " 8: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 12, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)]},\n",
       " 12: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 11, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, False)]},\n",
       " 13: {0: [(0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False)]},\n",
       " 14: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)]},\n",
       " 16: {0: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)]},\n",
       " 17: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)]},\n",
       " 18: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)]},\n",
       " 19: {0: [(1.0, 19, 0, True)],\n",
       "  1: [(1.0, 19, 0, True)],\n",
       "  2: [(1.0, 19, 0, True)],\n",
       "  3: [(1.0, 19, 0, True)]},\n",
       " 20: {0: [(0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 28, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True)]},\n",
       " 21: {0: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 20, 0.0, False)]},\n",
       " 22: {0: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 21, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 21, 0.0, False)]},\n",
       " 23: {0: [(0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 15, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)]},\n",
       " 24: {0: [(0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 16, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)]},\n",
       " 25: {0: [(0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 17, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)]},\n",
       " 26: {0: [(0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 18, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)]},\n",
       " 27: {0: [(0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 28, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 19, 0.0, True),\n",
       "   (0.3333333333333333, 26, 0.0, False)]},\n",
       " 28: {0: [(0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 27, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 20, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 20, 0.0, False),\n",
       "   (0.3333333333333333, 27, 0.0, False)]},\n",
       " 29: {0: [(1.0, 29, 0, True)],\n",
       "  1: [(1.0, 29, 0, True)],\n",
       "  2: [(1.0, 29, 0, True)],\n",
       "  3: [(1.0, 29, 0, True)]},\n",
       " 30: {0: [(0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 38, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 22, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True)]},\n",
       " 31: {0: [(0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 23, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)]},\n",
       " 32: {0: [(0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 24, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)]},\n",
       " 33: {0: [(0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, True),\n",
       "   (0.3333333333333333, 34, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 41, 0.0, True),\n",
       "   (0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 34, 0.0, False),\n",
       "   (0.3333333333333333, 25, 0.0, False),\n",
       "   (0.3333333333333333, 32, 0.0, False)]},\n",
       " 34: {0: [(0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 33, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 35, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 26, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 26, 0.0, False),\n",
       "   (0.3333333333333333, 33, 0.0, False)]},\n",
       " 35: {0: [(1.0, 35, 0, True)],\n",
       "  1: [(1.0, 35, 0, True)],\n",
       "  2: [(1.0, 35, 0, True)],\n",
       "  3: [(1.0, 35, 0, True)]},\n",
       " 36: {0: [(0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 44, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 28, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True)]},\n",
       " 37: {0: [(0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 29, 0.0, True),\n",
       "   (0.3333333333333333, 36, 0.0, False)]},\n",
       " 38: {0: [(0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 30, 0.0, False),\n",
       "   (0.3333333333333333, 37, 0.0, False)]},\n",
       " 39: {0: [(0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 38, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 31, 0.0, False),\n",
       "   (0.3333333333333333, 38, 0.0, False)]},\n",
       " 40: {0: [(0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 41, 0.0, True),\n",
       "   (0.3333333333333333, 32, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 41, 0.0, True),\n",
       "   (0.3333333333333333, 32, 0.0, False),\n",
       "   (0.3333333333333333, 40, 0.0, False)]},\n",
       " 41: {0: [(1.0, 41, 0, True)],\n",
       "  1: [(1.0, 41, 0, True)],\n",
       "  2: [(1.0, 41, 0, True)],\n",
       "  3: [(1.0, 41, 0, True)]},\n",
       " 42: {0: [(1.0, 42, 0, True)],\n",
       "  1: [(1.0, 42, 0, True)],\n",
       "  2: [(1.0, 42, 0, True)],\n",
       "  3: [(1.0, 42, 0, True)]},\n",
       " 43: {0: [(0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 51, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 35, 0.0, True),\n",
       "   (0.3333333333333333, 42, 0.0, True)]},\n",
       " 44: {0: [(0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 45, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 36, 0.0, False),\n",
       "   (0.3333333333333333, 43, 0.0, False)]},\n",
       " 45: {0: [(0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 44, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 37, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 37, 0.0, False),\n",
       "   (0.3333333333333333, 44, 0.0, False)]},\n",
       " 46: {0: [(1.0, 46, 0, True)],\n",
       "  1: [(1.0, 46, 0, True)],\n",
       "  2: [(1.0, 46, 0, True)],\n",
       "  3: [(1.0, 46, 0, True)]},\n",
       " 47: {0: [(0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 55, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 46, 0.0, True),\n",
       "   (0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 39, 0.0, False),\n",
       "   (0.3333333333333333, 46, 0.0, True)]},\n",
       " 48: {0: [(0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 40, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 40, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)]},\n",
       " 49: {0: [(1.0, 49, 0, True)],\n",
       "  1: [(1.0, 49, 0, True)],\n",
       "  2: [(1.0, 49, 0, True)],\n",
       "  3: [(1.0, 49, 0, True)]},\n",
       " 50: {0: [(0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 58, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 51, 0.0, False),\n",
       "   (0.3333333333333333, 42, 0.0, True),\n",
       "   (0.3333333333333333, 49, 0.0, True)]},\n",
       " 51: {0: [(0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 52, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 43, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 43, 0.0, False),\n",
       "   (0.3333333333333333, 50, 0.0, False)]},\n",
       " 52: {0: [(1.0, 52, 0, True)],\n",
       "  1: [(1.0, 52, 0, True)],\n",
       "  2: [(1.0, 52, 0, True)],\n",
       "  3: [(1.0, 52, 0, True)]},\n",
       " 53: {0: [(0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 61, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 45, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 45, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True)]},\n",
       " 54: {0: [(1.0, 54, 0, True)],\n",
       "  1: [(1.0, 54, 0, True)],\n",
       "  2: [(1.0, 54, 0, True)],\n",
       "  3: [(1.0, 54, 0, True)]},\n",
       " 55: {0: [(0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 63, 1.0, True)],\n",
       "  1: [(0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 63, 1.0, True),\n",
       "   (0.3333333333333333, 55, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 63, 1.0, True),\n",
       "   (0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 55, 0.0, False),\n",
       "   (0.3333333333333333, 47, 0.0, False),\n",
       "   (0.3333333333333333, 54, 0.0, True)]},\n",
       " 56: {0: [(0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 48, 0.0, False),\n",
       "   (0.3333333333333333, 56, 0.0, False)]},\n",
       " 57: {0: [(0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 56, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 49, 0.0, True),\n",
       "   (0.3333333333333333, 56, 0.0, False)]},\n",
       " 58: {0: [(0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 57, 0.0, False),\n",
       "   (0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 58, 0.0, False),\n",
       "   (0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 50, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 50, 0.0, False),\n",
       "   (0.3333333333333333, 57, 0.0, False)]},\n",
       " 59: {0: [(1.0, 59, 0, True)],\n",
       "  1: [(1.0, 59, 0, True)],\n",
       "  2: [(1.0, 59, 0, True)],\n",
       "  3: [(1.0, 59, 0, True)]},\n",
       " 60: {0: [(0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 60, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 59, 0.0, True),\n",
       "   (0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 61, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 52, 0.0, True),\n",
       "   (0.3333333333333333, 59, 0.0, True)]},\n",
       " 61: {0: [(0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 61, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 60, 0.0, False),\n",
       "   (0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 53, 0.0, False),\n",
       "   (0.3333333333333333, 60, 0.0, False)]},\n",
       " 62: {0: [(0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 61, 0.0, False),\n",
       "   (0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 62, 0.0, False),\n",
       "   (0.3333333333333333, 63, 1.0, True),\n",
       "   (0.3333333333333333, 54, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 63, 1.0, True),\n",
       "   (0.3333333333333333, 54, 0.0, True),\n",
       "   (0.3333333333333333, 61, 0.0, False)]},\n",
       " 63: {0: [(1.0, 63, 0, True)],\n",
       "  1: [(1.0, 63, 0, True)],\n",
       "  2: [(1.0, 63, 0, True)],\n",
       "  3: [(1.0, 63, 0, True)]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
