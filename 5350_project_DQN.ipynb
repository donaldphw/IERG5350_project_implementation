{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SailingEnvDQN import *\n",
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 200, # 2000\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map_name = \"8x8\", #16x16, 8x8 \n",
    "    is_slippery = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='SailingEnvDQN',\n",
    "             render=False):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    env = SailingEnvDQN(environment_config)\n",
    "    env.seed(seed)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        ep_step = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            ep_step += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "                wait(sleep=0.05)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(ep_step) \n",
    "    if render:\n",
    "        env.close()\n",
    "    return np.mean(rewards), np.mean(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    benchmark = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "#         print(\"Current iteration: {}\".format(i))\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward, step = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            result = dict(iteration=i, mean_reward=reward,mean_step=step)\n",
    "            benchmark.append(result)\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {} current mean step is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward, step,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats, benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        # [TODO] uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        \n",
    "        # [TODO] Build a sequential model with two layers.\n",
    "        # The first hidden layer has 100 hidden nodes, followed by\n",
    "        # a ReLU activation function.\n",
    "        # The second output layer take the activation vector, who has\n",
    "        # 100 elements, as input and return the action values.\n",
    "        # So the return values is a vector with num_actions elements.\n",
    "\n",
    "        self.action_value = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "        )\n",
    "#         self.action_value = nn.Sequential(\n",
    "#             nn.Linear(input_shape[0], 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, num_actions),\n",
    "#         )\n",
    "\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    \n",
    "# Test\n",
    "assert isinstance(PytorchModel(3, 7).action_value, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = dict(\n",
    "    env_name=\"SailingEnvDQN\",\n",
    "    max_iteration=500,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=10,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    seed=0,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env_name='SailingEnvDQN'\n",
    "        self.env = SailingEnvDQN(environment_config)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        self.eps = self.config['eps']\n",
    "        self.hidden_dim = self.config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "        self.initialize_parameters()\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.learn_start = self.config[\"learn_start\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.target_update_freq = self.config[\"target_update_freq\"]\n",
    "        self.clip_norm = self.config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.memory = ExperienceReplayMemory(self.config[\"memory_size\"])\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        input_shape = self.env.observation_space.shape\n",
    "\n",
    "        # [TODO] Initialize two network using PytorchModel class\n",
    "        self.network = PytorchModel(self.obs_dim,  self.action_dim)  # PytorchModel((3,), 7)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        self.target_network = PytorchModel(self.obs_dim, self.action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # [TODO] Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        if np.random.uniform(0, 1)  <= eps:\n",
    "            action = np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(values)\n",
    "\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # [TODO] Convert the output of neural network to numpy array\n",
    "        values = self.network(processed_state).detach().numpy()\n",
    "\n",
    "    \n",
    "        return values\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "#             print(next_state)\n",
    "\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                    \n",
    "                Q_t_plus_one = torch.max(self.target_network(next_state_batch).detach(), 1)[0] # to 1-D and get the tensor\n",
    "                \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # [TODO] Compute the target value of Q in batch.\n",
    "                \n",
    "                \n",
    "                #use (1.0 âˆ’ done) to determine if the game is ended or not\n",
    "                Q_target = (reward_batch + (1 - done_batch) * self.gamma * Q_t_plus_one).reshape(self.batch_size,) \n",
    "                assert Q_target.shape == (self.batch_size,)\n",
    "\n",
    "            self.network.train()\n",
    "\n",
    "\n",
    "            Q_t = self.network(state_batch).gather(dim = 1, index = action_batch.reshape(self.batch_size,1).long()).reshape(self.batch_size,)\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "#             print(\"{} steps has passed since last update. Now update the\"\n",
    "#                   \" parameter of the behavior policy. Current step: {}\".format(\n",
    "#                 self.step_since_update, self.total_step\n",
    "#             ))\n",
    "            self.step_since_update = 0\n",
    "            # [TODO] Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "   \n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "        G = 0.0\n",
    "        for i in range(tau, min(T, tau + n)):\n",
    "            G += rewards[i+1] * np.power(self.gamma, i - tau)\n",
    "        \n",
    "\n",
    "        \n",
    "        if tau + n < T:\n",
    "\n",
    "            G += self.gamma ** n * self.compute_values(processed_states[tau + n])[actions[tau + n]]\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "\n",
    "\n",
    "        loss_grad[[actions[tau]]] = -(G - self.compute_values(processed_states[tau])[actions[tau]])\n",
    "\n",
    "        value_grad = processed_states[tau].reshape(self.obs_dim, 1)\n",
    "        \n",
    "\n",
    "        assert loss_grad.shape == (self.act_dim, 1), loss_grad.shape\n",
    "        assert value_grad.shape == (self.obs_dim, 1), value_grad.shape\n",
    "\n",
    "        gradient = np.dot(loss_grad, value_grad.T).T\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        \n",
    "        # [TODO] apply the gradient to self.parameters\n",
    "        self.parameters -= self.learning_rate * gradient\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = merge_config(environment_config,\n",
    "                     pytorch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Python/3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.6s,+1.6s)\tIteration 0, current mean episode reward is -201.0 current mean step is 201.0. {'loss': nan, 'episode_len': 200.0}\n",
      "(3.1s,+1.5s)\tIteration 10, current mean episode reward is -201.0 current mean step is 201.0. {'loss': nan, 'episode_len': 97.0}\n",
      "Current memory contains 1000 transitions, start learning!\n",
      "(3.3s,+0.2s)\tIteration 20, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 628.0683, 'episode_len': 10.0}\n",
      "(4.9s,+1.6s)\tIteration 30, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 1776.3757, 'episode_len': 14.0}\n",
      "(5.2s,+0.3s)\tIteration 40, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 1494.4451, 'episode_len': 19.0}\n",
      "(6.8s,+1.7s)\tIteration 50, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 2018.5774, 'episode_len': 9.0}\n",
      "(8.6s,+1.8s)\tIteration 60, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 2792.452, 'episode_len': 20.0}\n",
      "(9.4s,+0.8s)\tIteration 70, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 772.269, 'episode_len': 13.0}\n",
      "(9.6s,+0.3s)\tIteration 80, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 162.3649, 'episode_len': 8.0}\n",
      "(9.9s,+0.3s)\tIteration 90, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 662.6488, 'episode_len': 8.0}\n",
      "(12.0s,+2.0s)\tIteration 100, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 852.6285, 'episode_len': 6.0}\n",
      "(12.3s,+0.3s)\tIteration 110, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 55.5387, 'episode_len': 4.0}\n",
      "(12.5s,+0.2s)\tIteration 120, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 756.651, 'episode_len': 8.0}\n",
      "(12.7s,+0.2s)\tIteration 130, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 51.8225, 'episode_len': 6.0}\n",
      "(12.9s,+0.2s)\tIteration 140, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 1422.3592, 'episode_len': 12.0}\n",
      "(13.1s,+0.2s)\tIteration 150, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 1095.3171, 'episode_len': 9.0}\n",
      "(13.2s,+0.2s)\tIteration 160, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 703.8599, 'episode_len': 6.0}\n",
      "(13.4s,+0.2s)\tIteration 170, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 391.1851, 'episode_len': 11.0}\n",
      "(13.6s,+0.2s)\tIteration 180, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 462.6771, 'episode_len': 8.0}\n",
      "(13.8s,+0.2s)\tIteration 190, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 327.4416, 'episode_len': 9.0}\n",
      "(14.0s,+0.2s)\tIteration 200, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 749.2871, 'episode_len': 11.0}\n",
      "(14.2s,+0.2s)\tIteration 210, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 802.5368, 'episode_len': 6.0}\n",
      "(14.3s,+0.2s)\tIteration 220, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 238.7221, 'episode_len': 6.0}\n",
      "(14.5s,+0.2s)\tIteration 230, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 189.0717, 'episode_len': 8.0}\n",
      "(14.7s,+0.2s)\tIteration 240, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 212.7267, 'episode_len': 8.0}\n",
      "(14.9s,+0.2s)\tIteration 250, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 277.5219, 'episode_len': 14.0}\n",
      "(15.1s,+0.2s)\tIteration 260, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 363.9648, 'episode_len': 9.0}\n",
      "(15.2s,+0.1s)\tIteration 270, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 262.3086, 'episode_len': 6.0}\n",
      "(15.4s,+0.1s)\tIteration 280, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 295.0856, 'episode_len': 6.0}\n",
      "(15.6s,+0.3s)\tIteration 290, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 306.9252, 'episode_len': 7.0}\n",
      "(15.8s,+0.2s)\tIteration 300, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 228.3936, 'episode_len': 5.0}\n",
      "(16.0s,+0.2s)\tIteration 310, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 240.3184, 'episode_len': 8.0}\n",
      "(16.2s,+0.2s)\tIteration 320, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 146.9384, 'episode_len': 6.0}\n",
      "(16.4s,+0.2s)\tIteration 330, current mean episode reward is 194.0 current mean step is 7.0. {'loss': 523.2062, 'episode_len': 10.0}\n",
      "(18.1s,+1.7s)\tIteration 340, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 198.7922, 'episode_len': 7.0}\n",
      "(19.9s,+1.8s)\tIteration 350, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 275.034, 'episode_len': 24.0}\n",
      "(21.8s,+1.8s)\tIteration 360, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 702.2802, 'episode_len': 19.0}\n",
      "(22.1s,+0.3s)\tIteration 370, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 576.6892, 'episode_len': 37.0}\n",
      "(22.5s,+0.4s)\tIteration 380, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 807.0819, 'episode_len': 9.0}\n",
      "(24.4s,+1.9s)\tIteration 390, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 1015.6256, 'episode_len': 32.0}\n",
      "(24.7s,+0.3s)\tIteration 400, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 953.2861, 'episode_len': 7.0}\n",
      "(25.0s,+0.3s)\tIteration 410, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 1114.1035, 'episode_len': 10.0}\n",
      "(25.2s,+0.2s)\tIteration 420, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 1557.9755, 'episode_len': 8.0}\n",
      "(26.9s,+1.7s)\tIteration 430, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 1615.6507, 'episode_len': 24.0}\n",
      "(27.3s,+0.4s)\tIteration 440, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 664.1945, 'episode_len': 14.0}\n",
      "(27.7s,+0.4s)\tIteration 450, current mean episode reward is 394.0 current mean step is 7.0. {'loss': 688.8324, 'episode_len': 20.0}\n",
      "(29.4s,+1.7s)\tIteration 460, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 1059.7974, 'episode_len': 26.0}\n",
      "(31.2s,+1.8s)\tIteration 470, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 2378.8051, 'episode_len': 25.0}\n",
      "(32.9s,+1.7s)\tIteration 480, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 2284.2301, 'episode_len': 12.0}\n",
      "(35.2s,+2.3s)\tIteration 490, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 2135.0948, 'episode_len': 17.0}\n",
      "(36.9s,+1.8s)\tIteration 500, current mean episode reward is -201.0 current mean step is 201.0. {'loss': 3803.8908, 'episode_len': 26.0}\n"
     ]
    }
   ],
   "source": [
    "pytorch_trainer, pytorch_stat, pytorch_benchmark = run(DQNTrainer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = \"benchmark/\"\n",
    "benchmark_col = [\"iteration\", \"mean_reward\", \"mean_step\"]\n",
    "benchmark_name = \"%sDQN_%s_%s.csv\" % (benchmark_dir, environment_config[\"map_name\"], environment_config[\"total_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(benchmark_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=benchmark_col)\n",
    "        writer.writeheader()\n",
    "        for data in pytorch_benchmark:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
