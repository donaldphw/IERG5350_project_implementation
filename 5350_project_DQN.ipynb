{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SailingEnvDQN import *\n",
    "import sys\n",
    "from contextlib import closing\n",
    "import random\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from utils import *\n",
    "from gym import utils, Env, spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.utils import seeding\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 2000, # 2000\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map_name = \"16x16\", #16x16, 8x8 \n",
    "    is_slippery = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='SailingEnvDQN',\n",
    "             render=False):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    env = SailingEnvDQN(environment_config)\n",
    "    env.seed(seed)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        ep_step = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            ep_step += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "                wait(sleep=0.05)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(ep_step) \n",
    "    if render:\n",
    "        env.close()\n",
    "    return np.mean(rewards), np.mean(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    benchmark = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "#         print(\"Current iteration: {}\".format(i))\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward, step = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            result = dict(iteration=i, mean_reward=reward,mean_step=step)\n",
    "            benchmark.append(result)\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {} current mean step is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward, step,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats, benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        # [TODO] uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        \n",
    "        # [TODO] Build a sequential model with two layers.\n",
    "        # The first hidden layer has 100 hidden nodes, followed by\n",
    "        # a ReLU activation function.\n",
    "        # The second output layer take the activation vector, who has\n",
    "        # 100 elements, as input and return the action values.\n",
    "        # So the return values is a vector with num_actions elements.\n",
    "\n",
    "        self.action_value = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "        )\n",
    "#         self.action_value = nn.Sequential(\n",
    "#             nn.Linear(input_shape[0], 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, num_actions),\n",
    "#         )\n",
    "\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    \n",
    "# Test\n",
    "assert isinstance(PytorchModel(3, 7).action_value, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = dict(\n",
    "    env_name=\"SailingEnvDQN\",\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=5,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    seed=0,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env_name='SailingEnvDQN'\n",
    "        self.env = SailingEnvDQN(environment_config)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        self.eps = self.config['eps']\n",
    "        self.hidden_dim = self.config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "        self.initialize_parameters()\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.learn_start = self.config[\"learn_start\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.target_update_freq = self.config[\"target_update_freq\"]\n",
    "        self.clip_norm = self.config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.memory = ExperienceReplayMemory(self.config[\"memory_size\"])\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        input_shape = self.env.observation_space.shape\n",
    "\n",
    "        # [TODO] Initialize two network using PytorchModel class\n",
    "        self.network = PytorchModel(self.obs_dim,  self.action_dim)  # PytorchModel((3,), 7)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        self.target_network = PytorchModel(self.obs_dim, self.action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # [TODO] Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        if np.random.uniform(0, 1)  <= eps:\n",
    "            action = np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(values)\n",
    "\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # [TODO] Convert the output of neural network to numpy array\n",
    "        values = self.network(processed_state).detach().numpy()\n",
    "\n",
    "    \n",
    "        return values\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "#             print(next_state)\n",
    "\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                    \n",
    "                Q_t_plus_one = torch.max(self.target_network(next_state_batch).detach(), 1)[0] # to 1-D and get the tensor\n",
    "                \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # [TODO] Compute the target value of Q in batch.\n",
    "                \n",
    "                \n",
    "                #use (1.0 − done) to determine if the game is ended or not\n",
    "                Q_target = (reward_batch + (1 - done_batch) * self.gamma * Q_t_plus_one).reshape(self.batch_size,) \n",
    "                assert Q_target.shape == (self.batch_size,)\n",
    "\n",
    "            self.network.train()\n",
    "\n",
    "\n",
    "            Q_t = self.network(state_batch).gather(dim = 1, index = action_batch.reshape(self.batch_size,1).long()).reshape(self.batch_size,)\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "#             print(\"{} steps has passed since last update. Now update the\"\n",
    "#                   \" parameter of the behavior policy. Current step: {}\".format(\n",
    "#                 self.step_since_update, self.total_step\n",
    "#             ))\n",
    "            self.step_since_update = 0\n",
    "            # [TODO] Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "   \n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "        G = 0.0\n",
    "        for i in range(tau, min(T, tau + n)):\n",
    "            G += rewards[i+1] * np.power(self.gamma, i - tau)\n",
    "        \n",
    "\n",
    "        \n",
    "        if tau + n < T:\n",
    "\n",
    "            G += self.gamma ** n * self.compute_values(processed_states[tau + n])[actions[tau + n]]\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "\n",
    "\n",
    "        loss_grad[[actions[tau]]] = -(G - self.compute_values(processed_states[tau])[actions[tau]])\n",
    "\n",
    "        value_grad = processed_states[tau].reshape(self.obs_dim, 1)\n",
    "        \n",
    "\n",
    "        assert loss_grad.shape == (self.act_dim, 1), loss_grad.shape\n",
    "        assert value_grad.shape == (self.obs_dim, 1), value_grad.shape\n",
    "\n",
    "        gradient = np.dot(loss_grad, value_grad.T).T\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        \n",
    "        # [TODO] apply the gradient to self.parameters\n",
    "        self.parameters -= self.learning_rate * gradient\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = merge_config(environment_config,\n",
    "                     pytorch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Python/3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2s,+0.2s)\tIteration 0, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 16.0}\n",
      "(0.3s,+0.2s)\tIteration 5, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 18.0}\n",
      "(0.5s,+0.2s)\tIteration 10, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 16.0}\n",
      "(0.7s,+0.2s)\tIteration 15, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 23.0}\n",
      "(0.9s,+0.2s)\tIteration 20, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 23.0}\n",
      "(1.1s,+0.2s)\tIteration 25, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 60.0}\n",
      "(1.2s,+0.2s)\tIteration 30, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': nan, 'episode_len': 14.0}\n",
      "Current memory contains 1000 transitions, start learning!\n",
      "(1.5s,+0.3s)\tIteration 35, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 285775.5104, 'episode_len': 67.0}\n",
      "(18.2s,+16.7s)\tIteration 40, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 232057.7741, 'episode_len': 45.0}\n",
      "(35.7s,+17.5s)\tIteration 45, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 367719.5811, 'episode_len': 27.0}\n",
      "(36.0s,+0.3s)\tIteration 50, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 316375.5532, 'episode_len': 44.0}\n",
      "(36.3s,+0.3s)\tIteration 55, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 462040.0499, 'episode_len': 14.0}\n",
      "(36.7s,+0.4s)\tIteration 60, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 459825.7337, 'episode_len': 54.0}\n",
      "(36.9s,+0.3s)\tIteration 65, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 165668.7605, 'episode_len': 24.0}\n",
      "(37.2s,+0.3s)\tIteration 70, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 621594.7405, 'episode_len': 16.0}\n",
      "(37.5s,+0.3s)\tIteration 75, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 70595.5229, 'episode_len': 7.0}\n",
      "(37.9s,+0.4s)\tIteration 80, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 509933.7984, 'episode_len': 28.0}\n",
      "(38.2s,+0.3s)\tIteration 85, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 882609.5143, 'episode_len': 15.0}\n",
      "(38.5s,+0.3s)\tIteration 90, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 711810.1988, 'episode_len': 20.0}\n",
      "(55.8s,+17.3s)\tIteration 95, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 432339.2706, 'episode_len': 42.0}\n",
      "(56.2s,+0.4s)\tIteration 100, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 574647.6285, 'episode_len': 18.0}\n",
      "(56.5s,+0.3s)\tIteration 105, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1170595.5586, 'episode_len': 25.0}\n",
      "(56.8s,+0.3s)\tIteration 110, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1252553.3471, 'episode_len': 16.0}\n",
      "(57.3s,+0.6s)\tIteration 115, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1372751.0538, 'episode_len': 10.0}\n",
      "(57.8s,+0.4s)\tIteration 120, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1266678.2112, 'episode_len': 20.0}\n",
      "(58.1s,+0.3s)\tIteration 125, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 998285.2208, 'episode_len': 27.0}\n",
      "(58.4s,+0.3s)\tIteration 130, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1366496.2194, 'episode_len': 14.0}\n",
      "(58.8s,+0.4s)\tIteration 135, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1052024.3665, 'episode_len': 17.0}\n",
      "(59.2s,+0.4s)\tIteration 140, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1480341.6011, 'episode_len': 16.0}\n",
      "(59.5s,+0.4s)\tIteration 145, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 947673.3637, 'episode_len': 21.0}\n",
      "(59.9s,+0.4s)\tIteration 150, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1169794.677, 'episode_len': 12.0}\n",
      "(60.3s,+0.4s)\tIteration 155, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1351479.9536, 'episode_len': 28.0}\n",
      "(60.7s,+0.4s)\tIteration 160, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1361577.8923, 'episode_len': 19.0}\n",
      "(61.2s,+0.5s)\tIteration 165, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1405121.617, 'episode_len': 24.0}\n",
      "(62.1s,+0.8s)\tIteration 170, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1043691.3519, 'episode_len': 90.0}\n",
      "(62.5s,+0.5s)\tIteration 175, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1412917.3607, 'episode_len': 13.0}\n",
      "(62.9s,+0.3s)\tIteration 180, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1232156.7627, 'episode_len': 31.0}\n",
      "(81.1s,+18.3s)\tIteration 185, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 601017.2203, 'episode_len': 18.0}\n",
      "(81.5s,+0.3s)\tIteration 190, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1355095.8865, 'episode_len': 16.0}\n",
      "(81.8s,+0.4s)\tIteration 195, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 977203.6142, 'episode_len': 41.0}\n",
      "(99.1s,+17.2s)\tIteration 200, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 1603191.1003, 'episode_len': 17.0}\n",
      "(117.4s,+18.3s)\tIteration 205, current mean episode reward is -2001.0 current mean step is 2001.0. {'loss': 1242157.4849, 'episode_len': 16.0}\n",
      "(117.8s,+0.4s)\tIteration 210, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1233379.2497, 'episode_len': 16.0}\n",
      "(118.1s,+0.3s)\tIteration 215, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1157915.0089, 'episode_len': 8.0}\n",
      "(118.5s,+0.4s)\tIteration 220, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1320504.1799, 'episode_len': 17.0}\n",
      "(119.0s,+0.5s)\tIteration 225, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1264836.668, 'episode_len': 20.0}\n",
      "(119.4s,+0.4s)\tIteration 230, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1813203.3618, 'episode_len': 9.0}\n",
      "(119.8s,+0.4s)\tIteration 235, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1519427.3646, 'episode_len': 24.0}\n",
      "(120.2s,+0.4s)\tIteration 240, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1632416.3365, 'episode_len': 8.0}\n",
      "(120.6s,+0.4s)\tIteration 245, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1517504.0035, 'episode_len': 16.0}\n",
      "(121.0s,+0.4s)\tIteration 250, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1308597.575, 'episode_len': 21.0}\n",
      "(121.3s,+0.4s)\tIteration 255, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 835403.3723, 'episode_len': 18.0}\n",
      "(121.8s,+0.4s)\tIteration 260, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 884393.4113, 'episode_len': 23.0}\n",
      "(122.1s,+0.4s)\tIteration 265, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1021510.5234, 'episode_len': 22.0}\n",
      "(122.6s,+0.4s)\tIteration 270, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1259267.1355, 'episode_len': 28.0}\n",
      "(123.0s,+0.5s)\tIteration 275, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1491043.5957, 'episode_len': 24.0}\n",
      "(123.4s,+0.4s)\tIteration 280, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1283421.8656, 'episode_len': 22.0}\n",
      "(123.8s,+0.4s)\tIteration 285, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 756337.4971, 'episode_len': 9.0}\n",
      "(124.2s,+0.4s)\tIteration 290, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1425842.0547, 'episode_len': 20.0}\n",
      "(124.5s,+0.3s)\tIteration 295, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1151933.2436, 'episode_len': 8.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124.9s,+0.4s)\tIteration 300, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 932502.7964, 'episode_len': 16.0}\n",
      "(125.3s,+0.3s)\tIteration 305, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 723561.1942, 'episode_len': 23.0}\n",
      "(125.6s,+0.4s)\tIteration 310, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 982203.0265, 'episode_len': 19.0}\n",
      "(126.1s,+0.4s)\tIteration 315, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 862420.4083, 'episode_len': 23.0}\n",
      "(126.4s,+0.3s)\tIteration 320, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 642311.2169, 'episode_len': 21.0}\n",
      "(126.7s,+0.3s)\tIteration 325, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1167939.5984, 'episode_len': 16.0}\n",
      "(127.1s,+0.4s)\tIteration 330, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 814268.8341, 'episode_len': 33.0}\n",
      "(127.5s,+0.4s)\tIteration 335, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 791380.5931, 'episode_len': 26.0}\n",
      "(127.8s,+0.3s)\tIteration 340, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 706456.9604, 'episode_len': 9.0}\n",
      "(128.2s,+0.4s)\tIteration 345, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 829014.4819, 'episode_len': 23.0}\n",
      "(128.5s,+0.3s)\tIteration 350, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 949174.6666, 'episode_len': 8.0}\n",
      "(128.9s,+0.4s)\tIteration 355, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 816854.2306, 'episode_len': 14.0}\n",
      "(129.3s,+0.4s)\tIteration 360, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 556962.8683, 'episode_len': 19.0}\n",
      "(129.6s,+0.3s)\tIteration 365, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 803285.8807, 'episode_len': 16.0}\n",
      "(129.8s,+0.3s)\tIteration 370, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1092806.804, 'episode_len': 16.0}\n",
      "(130.2s,+0.4s)\tIteration 375, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 898238.5602, 'episode_len': 18.0}\n",
      "(130.5s,+0.4s)\tIteration 380, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 672283.877, 'episode_len': 25.0}\n",
      "(130.9s,+0.4s)\tIteration 385, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 795942.2196, 'episode_len': 22.0}\n",
      "(131.3s,+0.4s)\tIteration 390, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 811823.4095, 'episode_len': 15.0}\n",
      "(131.7s,+0.4s)\tIteration 395, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 857838.5886, 'episode_len': 16.0}\n",
      "(132.1s,+0.4s)\tIteration 400, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 1270653.3896, 'episode_len': 10.0}\n",
      "(132.5s,+0.4s)\tIteration 405, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 845355.7894, 'episode_len': 18.0}\n",
      "(132.9s,+0.4s)\tIteration 410, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 644986.2975, 'episode_len': 22.0}\n",
      "(133.3s,+0.4s)\tIteration 415, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 812788.4436, 'episode_len': 22.0}\n",
      "(133.7s,+0.3s)\tIteration 420, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 641498.1902, 'episode_len': 18.0}\n",
      "(134.1s,+0.4s)\tIteration 425, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 747721.7344, 'episode_len': 16.0}\n",
      "(134.4s,+0.4s)\tIteration 430, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 609095.0982, 'episode_len': 17.0}\n",
      "(134.8s,+0.4s)\tIteration 435, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 799068.824, 'episode_len': 22.0}\n",
      "(135.2s,+0.4s)\tIteration 440, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 595772.6944, 'episode_len': 25.0}\n",
      "(135.6s,+0.4s)\tIteration 445, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 606819.284, 'episode_len': 20.0}\n",
      "(136.0s,+0.4s)\tIteration 450, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 663687.1979, 'episode_len': 26.0}\n",
      "(136.4s,+0.4s)\tIteration 455, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 682636.0903, 'episode_len': 29.0}\n",
      "(136.8s,+0.4s)\tIteration 460, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 419777.7287, 'episode_len': 24.0}\n",
      "(137.3s,+0.5s)\tIteration 465, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 467559.4019, 'episode_len': 31.0}\n",
      "(137.7s,+0.4s)\tIteration 470, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 227588.2538, 'episode_len': 11.0}\n",
      "(138.1s,+0.4s)\tIteration 475, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 454362.7878, 'episode_len': 31.0}\n",
      "(138.4s,+0.3s)\tIteration 480, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 377749.7204, 'episode_len': 18.0}\n",
      "(138.8s,+0.4s)\tIteration 485, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 329607.6721, 'episode_len': 17.0}\n",
      "(139.2s,+0.4s)\tIteration 490, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 411302.3022, 'episode_len': 14.0}\n",
      "(139.6s,+0.4s)\tIteration 495, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 283459.431, 'episode_len': 20.0}\n",
      "(140.0s,+0.3s)\tIteration 500, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 280755.6515, 'episode_len': 17.0}\n",
      "(140.4s,+0.4s)\tIteration 505, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 346075.8761, 'episode_len': 24.0}\n",
      "(140.7s,+0.4s)\tIteration 510, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 191199.1424, 'episode_len': 13.0}\n",
      "(141.1s,+0.4s)\tIteration 515, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 191782.4614, 'episode_len': 19.0}\n",
      "(141.4s,+0.3s)\tIteration 520, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 185895.6235, 'episode_len': 18.0}\n",
      "(141.8s,+0.4s)\tIteration 525, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 217693.1509, 'episode_len': 17.0}\n",
      "(142.1s,+0.4s)\tIteration 530, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 203663.5498, 'episode_len': 16.0}\n",
      "(142.4s,+0.3s)\tIteration 535, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 255576.3392, 'episode_len': 17.0}\n",
      "(142.8s,+0.4s)\tIteration 540, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 173781.3326, 'episode_len': 10.0}\n",
      "(143.2s,+0.4s)\tIteration 545, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 183213.6029, 'episode_len': 20.0}\n",
      "(143.6s,+0.4s)\tIteration 550, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 171770.0528, 'episode_len': 31.0}\n",
      "(143.9s,+0.4s)\tIteration 555, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 160333.785, 'episode_len': 17.0}\n",
      "(144.3s,+0.4s)\tIteration 560, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 141519.5739, 'episode_len': 20.0}\n",
      "(144.7s,+0.4s)\tIteration 565, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 156905.4462, 'episode_len': 18.0}\n",
      "(145.0s,+0.4s)\tIteration 570, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 146161.181, 'episode_len': 15.0}\n",
      "(145.4s,+0.4s)\tIteration 575, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 201115.7478, 'episode_len': 7.0}\n",
      "(145.8s,+0.4s)\tIteration 580, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 127317.7993, 'episode_len': 27.0}\n",
      "(146.2s,+0.4s)\tIteration 585, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 140366.0698, 'episode_len': 15.0}\n",
      "(146.5s,+0.4s)\tIteration 590, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 120094.8948, 'episode_len': 8.0}\n",
      "(147.0s,+0.4s)\tIteration 595, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 146654.8824, 'episode_len': 23.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147.4s,+0.4s)\tIteration 600, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 127724.4725, 'episode_len': 16.0}\n",
      "(147.8s,+0.4s)\tIteration 605, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 153008.6808, 'episode_len': 14.0}\n",
      "(148.2s,+0.4s)\tIteration 610, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': 136378.2201, 'episode_len': 19.0}\n",
      "(148.5s,+0.3s)\tIteration 615, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 135816.7661, 'episode_len': 21.0}\n",
      "(148.9s,+0.4s)\tIteration 620, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 157283.7169, 'episode_len': 21.0}\n",
      "(149.3s,+0.4s)\tIteration 625, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': 165751.9288, 'episode_len': 21.0}\n",
      "(149.7s,+0.4s)\tIteration 630, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': 145392.0577, 'episode_len': 22.0}\n",
      "(150.1s,+0.4s)\tIteration 635, current mean episode reward is 3986.0 current mean step is 15.0. {'loss': 130641.3823, 'episode_len': 23.0}\n",
      "(150.5s,+0.4s)\tIteration 640, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 218852.5229, 'episode_len': 28.0}\n",
      "(150.8s,+0.3s)\tIteration 645, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 237050.0116, 'episode_len': 18.0}\n",
      "(151.2s,+0.4s)\tIteration 650, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 193304.4325, 'episode_len': 8.0}\n",
      "(151.5s,+0.4s)\tIteration 655, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 138664.8961, 'episode_len': 21.0}\n",
      "(151.9s,+0.3s)\tIteration 660, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 264660.1661, 'episode_len': 21.0}\n",
      "(152.2s,+0.3s)\tIteration 665, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 218974.6658, 'episode_len': 9.0}\n",
      "(152.5s,+0.3s)\tIteration 670, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 250563.9305, 'episode_len': 26.0}\n",
      "(152.9s,+0.4s)\tIteration 675, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 261781.1901, 'episode_len': 21.0}\n",
      "(153.3s,+0.4s)\tIteration 680, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 203842.4604, 'episode_len': 14.0}\n",
      "(153.7s,+0.4s)\tIteration 685, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 172900.9734, 'episode_len': 21.0}\n",
      "(154.1s,+0.4s)\tIteration 690, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 324930.8851, 'episode_len': 34.0}\n",
      "(154.5s,+0.4s)\tIteration 695, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 298920.0681, 'episode_len': 30.0}\n",
      "(154.8s,+0.4s)\tIteration 700, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 292523.703, 'episode_len': 28.0}\n",
      "(155.2s,+0.4s)\tIteration 705, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 297252.7538, 'episode_len': 24.0}\n",
      "(155.6s,+0.4s)\tIteration 710, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 238699.4641, 'episode_len': 18.0}\n",
      "(155.9s,+0.3s)\tIteration 715, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 256528.5134, 'episode_len': 7.0}\n",
      "(156.3s,+0.4s)\tIteration 720, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 170364.971, 'episode_len': 7.0}\n",
      "(156.7s,+0.4s)\tIteration 725, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 301072.7061, 'episode_len': 20.0}\n",
      "(157.2s,+0.5s)\tIteration 730, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 412112.7265, 'episode_len': 19.0}\n",
      "(157.6s,+0.3s)\tIteration 735, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 492772.8665, 'episode_len': 18.0}\n",
      "(157.9s,+0.4s)\tIteration 740, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 520527.8401, 'episode_len': 33.0}\n",
      "(158.3s,+0.4s)\tIteration 745, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 393118.5489, 'episode_len': 23.0}\n",
      "(158.7s,+0.4s)\tIteration 750, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 631476.2821, 'episode_len': 9.0}\n",
      "(159.0s,+0.4s)\tIteration 755, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 511556.5748, 'episode_len': 14.0}\n",
      "(159.4s,+0.3s)\tIteration 760, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 396227.303, 'episode_len': 16.0}\n",
      "(159.7s,+0.3s)\tIteration 765, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 682497.9265, 'episode_len': 8.0}\n",
      "(160.1s,+0.4s)\tIteration 770, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 450784.8611, 'episode_len': 25.0}\n",
      "(160.6s,+0.4s)\tIteration 775, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 416536.8813, 'episode_len': 30.0}\n",
      "(161.0s,+0.4s)\tIteration 780, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 486305.7781, 'episode_len': 25.0}\n",
      "(161.4s,+0.4s)\tIteration 785, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 289312.7994, 'episode_len': 10.0}\n",
      "(161.8s,+0.4s)\tIteration 790, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 777237.6967, 'episode_len': 22.0}\n",
      "(162.2s,+0.4s)\tIteration 795, current mean episode reward is 7986.0 current mean step is 15.0. {'loss': 448520.2042, 'episode_len': 7.0}\n"
     ]
    }
   ],
   "source": [
    "pytorch_trainer, pytorch_stat, pytorch_benchmark = run(DQNTrainer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = \"benchmark/\"\n",
    "benchmark_col = [\"iteration\", \"mean_reward\", \"mean_step\"]\n",
    "benchmark_name = \"%sDQN_%s_%s.csv\" % (benchmark_dir, environment_config[\"map_name\"], environment_config[\"total_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(benchmark_name, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=benchmark_col)\n",
    "        writer.writeheader()\n",
    "        for data in pytorch_benchmark:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
